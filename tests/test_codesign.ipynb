{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../txf_design-space/')\n",
    "sys.path.append('../txf_design-space/flexibert')\n",
    "sys.path.append('../protran/boshnas/boshnas/')\n",
    "sys.path.append('../global_search/utils')\n",
    "sys.path.append('../')\n",
    "\n",
    "import re\n",
    "import yaml\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import shlex\n",
    "import shutil\n",
    "import argparse\n",
    "import subprocess\n",
    "import collections\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from matplotlib import pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "from embeddings.utils import graph_util, print_util as pu\n",
    "\n",
    "sys.path.append('../txf_design-space/transformers/src/transformers')\n",
    "import embedding_util\n",
    "\n",
    "from boshnas import BOSHNAS\n",
    "from boshnas_2inp import BOSHNAS as BOSHCODE\n",
    "from acq import gosh_acq as acq\n",
    "\n",
    "from transformers import BertModel\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "from transformers.models.bert.modeling_modular_bert import BertModelModular, BertForMaskedLMModular, BertForSequenceClassificationModular\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.linear_model import Ridge, LinearRegression, RANSACRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, ndcg_score\n",
    "\n",
    "from six.moves import cPickle as pickle\n",
    "from scipy.interpolate import griddata\n",
    "from sklearn.tree import plot_tree\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "\n",
    "import lightgbm\n",
    "\n",
    "PREFIX_CHECKPOINT_DIR = \"checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toggles and constants\n",
    "K_PRETRAIN = 0.5; K_LATENCY = 0.2; K_ENERGY = 0.2; K_PEAK_POWER = 0.1\n",
    "MAX_PRETRAIN = 4; MAX_LATENCY = 1000; MAX_ENERGY = 2000; MAX_PEAK_POWER = 300\n",
    "PERFORMANCE_PATIENCE = 5\n",
    "RANDOM_SAMPLES = 100\n",
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load surrogate model for pre-train loss\n",
    "gbdtr = pickle.load(open('../global_search/dataset/surrogate_models/pretrain.pkl', 'rb'))\n",
    "\n",
    "# Prepare stochastic surrogate model\n",
    "def predict_loss(gbdtr, X):\n",
    "    mean = float(gbdtr.predict(X))\n",
    "    dt_preds = []\n",
    "    for estimator in gbdtr.estimators_:\n",
    "        pred = estimator[0].predict(X)\n",
    "        dt_preds.append(pred)\n",
    "    std = float(np.std(np.array(dt_preds)))\n",
    "    return float(np.random.normal(mean, std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-design dataset created of size: 782\n",
      "Loaded co-design dataset\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset of transformer-device pairs (from common initial Sobol samples and predicted accuracies)\n",
    "devices = ['a100', 'cpu_m1-pro', 'gpu_m1-pro', 'cpu_rpi', 'npu_ncs', 'cpu_jetson-nano', 'gpu_jetson-nano']\n",
    "device_names = ['A100', 'Apple M1 CPU', 'Apple M1 GPU', 'Raspberry Pi CPU', 'Intel NCS NPU', 'Jetson Nano CPU', \n",
    "           'Jetson Nano GPU']\n",
    "device_dirs = ['results_gpu_a100_b128_r3', 'results_cpu_m1-pro_b32_r1', 'results_gpu_m1-pro_b32_r3',\n",
    "                'results_cpu_rpi_b1_r1', 'results_npu_ncs_b1_r1', 'results_cpu_jetson-nano_b1_r1',\n",
    "                'results_gpu_jetson-nano_b1_r1']\n",
    "\n",
    "# Dataset for transformer pre-train loss\n",
    "dataset_txf = json.load(open('../global_search/dataset/dataset.json'))\n",
    "dataset = {}\n",
    "\n",
    "# Surrogate model for pre-train loss\n",
    "gbdtr = pickle.load(open('../global_search/dataset/surrogate_models/pretrain.pkl', 'rb'))\n",
    "\n",
    "for i, device_dir in enumerate(device_dirs):\n",
    "    dataset_device = json.load(\n",
    "        open(f'../protran/energy_profiler/results/{device_dir}/dataset/dataset.json', 'r'))\n",
    "    for model in dataset_device.keys():\n",
    "        pair = model + '_' + devices[i]\n",
    "        dataset[pair] = deepcopy(dataset_device[model])\n",
    "        dataset[pair]['embedding_txf'] = dataset_device[model]['embedding']; del dataset[pair]['embedding']\n",
    "        embedding_device = [0 for _ in devices]; embedding_device[i] = 1\n",
    "        dataset[pair]['embedding_device'] = str(embedding_device)\n",
    "        if model in dataset_txf.keys() and dataset_txf[model]['pretrain']['loss'] is not None and \\\n",
    "            dataset_txf[model]['pretrain']['loss'] < 7:\n",
    "            dataset[pair]['performance']['pretrain_loss'] = dataset_txf[model]['pretrain']['loss']\n",
    "        else:\n",
    "            dataset[pair]['performance']['pretrain_loss'] = \\\n",
    "                float(predict_loss(gbdtr, np.array(eval(dataset[pair]['embedding_txf'])).reshape(1, -1)))\n",
    "        \n",
    "print(f'Co-design dataset created of size: {len(dataset)}')\n",
    "\n",
    "if not os.path.exists('../co-design/dataset/dataset.json'):\n",
    "    json.dump(dataset, open('../co-design/dataset/dataset.json', 'w+'))\n",
    "    print('Saved co-design dataset')\n",
    "else:\n",
    "    dataset = json.load(open('../co-design/dataset/dataset.json', 'r'))\n",
    "    print('Loaded co-design dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min. pre-train loss:  1.939. \tMax. pre-train loss:  3.599\n",
      "Min. latency:  0.003. \t\tMax. latency:  814.751\n",
      "Min. energy:  0.042. \t\tMax. energy:  1864.779\n",
      "Min. peak power:  2.008. \tMax. peak power:  282.700\n"
     ]
    }
   ],
   "source": [
    "# Convert dataset to tabular format\n",
    "X_txf, X_device, y_pretrain, y_latency, y_energy, y_peak_power = [], [], [], [], [], []\n",
    "for pair in dataset.keys():\n",
    "    X_txf.append(eval(dataset[pair]['embedding_txf']))\n",
    "    X_device.append(eval(dataset[pair]['embedding_device']))\n",
    "    y_pretrain.append(dataset[pair]['performance']['pretrain_loss'])\n",
    "    y_latency.append(dataset[pair]['performance']['latency'])\n",
    "    y_energy.append(dataset[pair]['performance']['energy'])\n",
    "    y_peak_power.append(dataset[pair]['performance']['peak_power'])\n",
    "    \n",
    "X_txf, X_device = np.array(X_txf), np.array(X_device)\n",
    "    \n",
    "# Get performance bounds\n",
    "print(f'Min. pre-train loss: {min(y_pretrain) : 0.3f}. \\tMax. pre-train loss: {max(y_pretrain) : 0.3f}')\n",
    "print(f'Min. latency: {min(y_latency) : 0.3f}. \\t\\tMax. latency: {max(y_latency) : 0.3f}')\n",
    "print(f'Min. energy: {min(y_energy) : 0.3f}. \\t\\tMax. energy: {max(y_energy) : 0.3f}')\n",
    "print(f'Min. peak power: {min(y_peak_power) : 0.3f}. \\tMax. peak power: {max(y_peak_power) : 0.3f}')\n",
    "\n",
    "# Get input bounds\n",
    "bounds_device = (np.min(X_device, axis=0), np.max(X_device, axis=0))\n",
    "design_space = yaml.safe_load(open('../global_search/design_space/design_space.yaml'))\n",
    "bounds_txf = embedding_util.get_embedding_bounds(design_space, 'all')\n",
    "bounds_txf = (np.array([bound[0] for bound in bounds_txf]), \\\n",
    "                    np.array([bound[1] for bound in bounds_txf]))\n",
    "\n",
    "# Get convex combination output\n",
    "y = []\n",
    "for i in range(len(y_pretrain)):\n",
    "    performance = K_PRETRAIN * (1 - y_pretrain[i] / MAX_PRETRAIN) + \\\n",
    "                  K_LATENCY * (1 - y_latency[i] / MAX_LATENCY) + \\\n",
    "                  K_ENERGY * (1 - y_energy[i] / MAX_ENERGY) + \\\n",
    "                  K_PEAK_POWER * (1 - y_peak_power[i] / MAX_PEAK_POWER)\n",
    "    y.append(1 - performance)\n",
    "    \n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training teacher model: 100%|██████████| 200/200 [02:30<00:00,  1.33it/s]\n",
      "Training student model: 100%|██████████| 200/200 [11:52<00:00,  3.56s/it]\n",
      "Training NPN model: 100%|██████████| 200/200 [09:04<00:00,  2.72s/it]\n"
     ]
    }
   ],
   "source": [
    "# Train BOSHCODE on preliminary dataset\n",
    "ckpt_exists = os.path.exists('../co-design/dataset/surrogate_model/teacher.pkl')\n",
    "surrogate_model = BOSHCODE(input_dim1=X_txf.shape[1],\n",
    "                           input_dim2=X_device.shape[1],\n",
    "                           bounds1=bounds_txf,\n",
    "                           bounds2=bounds_device,\n",
    "                           trust_region=False,\n",
    "                           second_order=True,\n",
    "                           parallel=True,\n",
    "                           model_aleatoric=True,\n",
    "                           save_path='../co-design/dataset/surrogate_model/',\n",
    "                           pretrained=ckpt_exists)\n",
    "\n",
    "max_loss = np.amax(y)\n",
    "y = y/max_loss\n",
    "\n",
    "if not ckpt_exists: train_error = surrogate_model.train(X_txf, X_device, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load surrogate models for latency, energy and peak power\n",
    "latency_models = [pickle.load(\n",
    "    open(f'../protran/energy_profiler/results/{device_dir}/dataset/surrogate_models/latency.pkl', 'rb')) \\\n",
    "    for device_dir in device_dirs] \n",
    "energy_models = [pickle.load(\n",
    "    open(f'../protran/energy_profiler/results/{device_dir}/dataset/surrogate_models/energy.pkl', 'rb')) \\\n",
    "    for device_dir in device_dirs] \n",
    "peak_power_models = [pickle.load(\n",
    "    open(f'../protran/energy_profiler/results/{device_dir}/dataset/surrogate_models/peak_power.pkl', 'rb')) \\\n",
    "    for device_dir in device_dirs] \n",
    "max_values = [json.load(\n",
    "    open(f'../protran/energy_profiler/results/{device_dir}/dataset/surrogate_models/max_values.json', 'rb')) \\\n",
    "    for device_dir in device_dirs] \n",
    "\n",
    "# Prepare stochastic surrogate model\n",
    "def predict_hw_performance(latency_models, energy_models, peak_power_models, max_values, X_txf, X_device):\n",
    "    assert X_device.shape[0] == 1\n",
    "    device_idx = int(np.argmax(np.squeeze(X_device)))\n",
    "    latency_mean = float(latency_models[device_idx].predict(X_txf) * max_values[device_idx]['max_latency'])\n",
    "    energy_mean = float(energy_models[device_idx].predict(X_txf) * max_values[device_idx]['max_energy'])\n",
    "    peak_power_mean = float(peak_power_models[device_idx].predict(X_txf) * max_values[device_idx]['max_peak_power'])\n",
    "    latency_preds, energy_preds, peak_power_preds = [], [], []\n",
    "    for latency_estimator, energy_estimator, peak_power_estimator in \\\n",
    "    zip(latency_models[device_idx].estimators_, energy_models[device_idx].estimators_, \\\n",
    "    peak_power_models[device_idx].estimators_):\n",
    "        latency_preds.append(latency_estimator.predict(X_txf) * max_values[device_idx]['max_latency'])\n",
    "        energy_preds.append(energy_estimator.predict(X_txf) * max_values[device_idx]['max_energy'])\n",
    "        peak_power_preds.append(peak_power_estimator.predict(X_txf) * max_values[device_idx]['max_peak_power'])\n",
    "    latency_std = float(np.std(np.array(latency_preds)))\n",
    "    energy_std = float(np.std(np.array(energy_preds)))\n",
    "    peak_power_std = float(np.std(np.array(peak_power_preds)))\n",
    "    return float(np.random.normal(latency_mean, latency_std)), float(np.random.normal(energy_mean, energy_std)), \\\n",
    "        float(np.random.normal(peak_power_mean, peak_power_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random samples for devices\n",
    "def get_device_samples(num_devices, num_samples):\n",
    "    samples = []\n",
    "    for _ in range(num_samples):\n",
    "        device_idx = np.random.randint(num_devices)\n",
    "        device = np.zeros(num_devices)\n",
    "        device[device_idx] = 1\n",
    "        samples.append(device)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run co-design with exploration on random samples\n",
    "random_samples = embedding_util.get_samples(design_space, \n",
    "                                            num_samples=RANDOM_SAMPLES, \n",
    "                                            sampling_method='Random', \n",
    "                                            debug=False)\n",
    "\n",
    "best_performance, old_best_performance, same_performance, itn = np.amin(y), np.inf, 0, 0\n",
    "while same_performance < PERFORMANCE_PATIENCE:\n",
    "    random_txf_samples = embedding_util.get_samples(design_space, \n",
    "                                            num_samples=RANDOM_SAMPLES, \n",
    "                                            sampling_method='Random', \n",
    "                                            debug=False)\n",
    "    random_txf_samples = [random_txf_samples[model]['embedding'] for model in random_txf_samples.keys()]\n",
    "    random_device_samples = get_device_samples(num_devices=len(devices), num_samples=RANDOM_SAMPLES)\n",
    "    \n",
    "    random_samples = [(np.array(random_txf_samples[i]), \n",
    "                       random_device_samples[i]) for i in range(len(random_txf_samples))]\n",
    "    \n",
    "    # Get queries using GOBI\n",
    "    query_indices = surrogate_model.get_queries(x=random_samples, k=K, explore_type='ucb', use_al=True)\n",
    "    \n",
    "    for i in set(query_indices):\n",
    "        X_txf_new, X_device_new = random_samples[i]\n",
    "        y_pretrain_new = predict_loss(gbdtr, X_txf_new.reshape(1, -1))\n",
    "        y_latency_new, y_energy_new, y_peak_power_new = predict_hw_performance(latency_models, energy_models, \n",
    "                                                            peak_power_models, max_values, \n",
    "                                                            X_txf_new.reshape(1, -1), X_device_new.reshape(1, -1))\n",
    "        performance = K_PRETRAIN * (1 - y_pretrain_new / MAX_PRETRAIN) + \\\n",
    "                  K_LATENCY * (1 - y_latency_new / MAX_LATENCY) + \\\n",
    "                  K_ENERGY * (1 - y_energy_new / MAX_ENERGY) + \\\n",
    "                  K_PEAK_POWER * (1 - y_peak_power_new / MAX_PEAK_POWER)\n",
    "        \n",
    "        y_new = np.array([1 - performance])\n",
    "        y = np.concatenate((y, y_new))\n",
    "        \n",
    "        X_txf, X_device = np.concatenate((X_txf, X_txf_new.reshape(1, -1))), \\\n",
    "            np.concatenate((X_device, X_device_new.reshape(1, -1)))\n",
    "    \n",
    "    best_performance = np.amin(y)\n",
    "    print(f'Current iteration: {itn}. \\tBest performance: {best_performance}')\n",
    "    \n",
    "    # Update same_performance to check convergence\n",
    "    if best_performance == old_best_performance:\n",
    "        same_performance += 1\n",
    "\n",
    "    old_best_performance = best_performance\n",
    "    \n",
    "    # Train model on expanded dataset\n",
    "    train_error = surrogate_model.train(X_txf, X_device, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txf_design-space [~/.conda/envs/txf_design-space/]",
   "language": "python",
   "name": "conda_txf_design-space"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
