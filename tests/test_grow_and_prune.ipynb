{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../../txf_design-space/embeddings/utils')\n",
    "sys.path.append('../../txf_design-space/flexibert')\n",
    "sys.path.append('../grow_and_prune/')\n",
    "import graph_util\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import shlex\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from roberta_pretraining import pretrain\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, interleave_datasets, load_from_disk\n",
    "from transformers.models.bert.modeling_modular_bert import BertModelModular, BertForMaskedLMModular\n",
    "from transformers import BertModel\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "\n",
    "import argparse\n",
    "import pretrain_model\n",
    "\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_MASKED_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.attention.self.distance_embedding.weight', 'roberta.encoder.layer.1.attention.self.distance_embedding.weight', 'roberta.encoder.layer.2.attention.self.distance_embedding.weight', 'roberta.encoder.layer.3.attention.self.distance_embedding.weight', 'roberta.encoder.layer.4.attention.self.distance_embedding.weight', 'roberta.encoder.layer.5.attention.self.distance_embedding.weight', 'roberta.encoder.layer.6.attention.self.distance_embedding.weight', 'roberta.encoder.layer.7.attention.self.distance_embedding.weight', 'roberta.encoder.layer.8.attention.self.distance_embedding.weight', 'roberta.encoder.layer.9.attention.self.distance_embedding.weight', 'roberta.encoder.layer.10.attention.self.distance_embedding.weight', 'roberta.encoder.layer.11.attention.self.distance_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash of BERT-Base: 07aaba14d29455a984e2aef6312a8870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../models/07aaba14d29455a984e2aef6312a8870/tokenizer_config.json',\n",
       " '../models/07aaba14d29455a984e2aef6312a8870/special_tokens_map.json',\n",
       " '../models/07aaba14d29455a984e2aef6312a8870/vocab.json',\n",
       " '../models/07aaba14d29455a984e2aef6312a8870/merges.txt',\n",
       " '../models/07aaba14d29455a984e2aef6312a8870/added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transfer weights of BERT-Base to heterogeneous counterpart\n",
    "roberta_base = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "model_dict_hetero = {'l': 12, 'o': [['sa_sdp_64']*12]*12, 'h': [768]*12, 'f': [[3072]]*12}\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "config = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "\n",
    "config.from_model_dict_hetero(model_dict_hetero)\n",
    "\n",
    "model = BertModelModular(config)\n",
    "\n",
    "model_state_dict = model.state_dict()\n",
    "model_state_dict.update(roberta_base.state_dict())\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "model_graph = graph_util.model_dict_to_graph(model_dict_hetero)\n",
    "model_hash = graph_util.hash_graph(*model_graph)\n",
    "output_dir = '../models/'+model_hash+'/'\n",
    "\n",
    "print(f'Hash of BERT-Base: {model_hash}')\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model dict for BERT-Mini like model\n",
    "model_dict = {'l': 4, 'o': ['sa']*4, 'h': [256]*4, 'n': [4]*4, 'f': [[1024]]*4, 'p': ['sdp']*4}\n",
    "model_dict_hetero = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'c_5_64', 'sa_sdp_64']]*4, \n",
    "                     'h': [256]*4, 'f': [[1024]]*4}\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "\n",
    "config = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "# config.from_model_dict(model_dict)\n",
    "\n",
    "config.from_model_dict_hetero(model_dict_hetero)\n",
    "\n",
    "model = BertForMaskedLMModular(config)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfering embeddings using mode: OD\n",
      "Checking layer 0...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: c_13_64\n",
      "\tTransfering attention head 3: sa_wma_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 1...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: c_13_64\n",
      "\tTransfering attention head 3: sa_wma_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 2...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: c_13_64\n",
      "\tTransfering attention head 3: sa_wma_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 3...\n",
      "\tTransfering distance embeddings using mode: OD\n",
      "\tTransfering attention head 0: sa_sdp_32\n",
      "\tTransfering attention head 1: sa_sdp_32\n",
      "\tTransfering attention head 2: c_5_32\n",
      "\tTransfering attention head 3: sa_wma_32\n",
      "\tTransfering feed-forward layer 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test load_from_source()\n",
    "# model_dict_hetero2 = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']]*3 + \\\n",
    "#                                    [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_wma_64']], \n",
    "#                       'h': [256]*4, 'f': [[1024, 1024]]*4}\n",
    "model_dict_hetero2 = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'c_13_64', 'sa_wma_64']]*3 + \\\n",
    "                                    [['sa_sdp_32', 'sa_sdp_32', 'c_5_32', 'sa_wma_32']], \n",
    "                     'h': [128, 256, 256, 512], 'f': [[2048, 256]]*4}\n",
    "# model_dict_hetero2 = {'l': 4, 'o': [['sa_sdp_32', 'sa_sdp_32', 'c_13_32', 'sa_sdp_32']]*4, \n",
    "#                      'h': [256]*4, 'f': [[1024]]*4}\n",
    "\n",
    "config2 = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "config2.from_model_dict_hetero(model_dict_hetero2)\n",
    "\n",
    "model2 = BertForMaskedLMModular(config2, transfer_mode='OD')\n",
    "\n",
    "# print(model2)\n",
    "\n",
    "model2.bert.load_model_from_source(model.bert, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.encoder.layer[0].attention.self.conv_kernel_layer0.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 128)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test RP\n",
    "from sklearn import random_projection\n",
    "\n",
    "matrix = torch.rand(1000, 1000)\n",
    "matrix_numpy = matrix.cpu().numpy()\n",
    "\n",
    "rp = random_projection.GaussianRandomProjection(128)\n",
    "matrix_numpy_new = rp.fit_transform(matrix_numpy)\n",
    "\n",
    "matrix_numpy_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RP with torch\n",
    "import torch.nn as nn\n",
    "from torch.tensor import Tensor\n",
    "\n",
    "model.bert.encoder.layer[0].attention.self.query.weight = \\\n",
    "    nn.parameter.Parameter(\n",
    "        torch.from_numpy(\n",
    "            rp.fit_transform(model.bert.encoder.layer[0].attention.self.query.weight.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), ['input', 'sa_h128_p-sdp', 'sa_h128_p-sdp', 'add_norm', 'f512', 'f512', 'add_norm', 'sa_h128_p-sdp', 'sa_h128_p-sdp', 'add_norm', 'f512', 'f512', 'add_norm', 'output'])\n"
     ]
    }
   ],
   "source": [
    "# Test graph generation from model dict\n",
    "model_dict = {'l': 2, 'o': ['sa']*2, 'h': [128]*2, 'n': [2]*2, 'f': [[512, 512]]*2, 'p': ['sdp']*2}\n",
    "\n",
    "model_graph = graph_util.model_dict_to_graph(model_dict)\n",
    "\n",
    "print(model_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50ef6167847c070825f29805f4e9cd35'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test hashing for model_dict_hetero\n",
    "model_dict_hetero = {'l': 4, 'o': [['l_dft_64', 'sa_wma_64', 'sa_wma_64', 'c_9_64']]*4, \n",
    "                     'h': [256]*4, 'n': [4]*4, 'f': [[1024]]*4}\n",
    "\n",
    "model_graph = graph_util.model_dict_to_graph(model_dict_hetero)\n",
    "\n",
    "graph_util.hash_graph(*model_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_args(seed, max_steps, per_gpu_batch_size, output_dir, local_rank):\n",
    "    a = \"--seed {} \\\n",
    "    --do_train \\\n",
    "    --max_seq_length 512 \\\n",
    "    --per_gpu_train_batch_size {} \\\n",
    "    --max_steps {} \\\n",
    "    --adam_epsilon 1e-6 \\\n",
    "    --adam_beta2 0.98 \\\n",
    "    --learning_rate 1000e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --warmup_steps 10000 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --output_dir {} \\\n",
    "    --local_rank {} \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 10 \\\n",
    "        \".format(seed, per_gpu_batch_size, max_steps, output_dir, local_rank)\n",
    "    return shlex.split(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-Mini's model hash: 7f3d35e33a37f8d6a55d3d3ee339d7ba\n",
      "02/16/2022 17:18:54 - WARNING - roberta_pretraining -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "02/16/2022 17:18:54 - INFO - roberta_pretraining -   Training/evaluation parameters TrainingArguments(output_dir=./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/, overwrite_output_dir=False, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, eval_accumulation_steps=None, learning_rate=0.1, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.98, adam_epsilon=1e-06, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=100, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=10000, logging_dir=runs/Feb16_17-18-54_della-r2c3n1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=10, save_strategy=IntervalStrategy.STEPS, save_steps=10, save_total_limit=2, no_cuda=False, seed=0, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name=./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=0, mp_parameters=)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1647] 2022-02-16 17:18:54,444 >> Didn't find file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1647] 2022-02-16 17:18:54,446 >> Didn't find file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-16 17:18:54,447 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-16 17:18:54,448 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-16 17:18:54,449 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-16 17:18:54,451 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-16 17:18:54,452 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-16 17:18:54,453 >> loading file None\n",
      "[INFO|trainer.py:375] 2022-02-16 17:19:00,998 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:488] 2022-02-16 17:19:00,999 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLMModular.forward` and have been ignored: special_tokens_mask.\n",
      "[WARNING|training_args.py:633] 2022-02-16 17:19:01,000 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:633] 2022-02-16 17:19:01,002 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:1011] 2022-02-16 17:19:01,002 >> ***** Running training *****\n",
      "[INFO|trainer.py:1012] 2022-02-16 17:19:01,002 >>   Num examples = 3149357\n",
      "[INFO|trainer.py:1013] 2022-02-16 17:19:01,003 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1014] 2022-02-16 17:19:01,003 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1015] 2022-02-16 17:19:01,003 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "[INFO|trainer.py:1016] 2022-02-16 17:19:01,003 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1017] 2022-02-16 17:19:01,004 >>   Total optimization steps = 100\n"
     ]
    }
   ],
   "source": [
    "# Test loss from manual GP\n",
    "model_dict_bert_mini = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']]*4, \n",
    "                     'h': [256]*4, 'f': [[1024]]*4}\n",
    "model_graph_bert_mini = graph_util.model_dict_to_graph(model_dict_bert_mini)\n",
    "model_hash_bert_mini = graph_util.hash_graph(*model_graph_bert_mini)\n",
    "\n",
    "print(f'BERT-Mini\\'s model hash: {model_hash_bert_mini}')\n",
    "\n",
    "output_dir_bert_mini = f'./models/{model_hash_bert_mini}/'\n",
    "\n",
    "args_train = get_training_args(0, 100, 32, output_dir_bert_mini, -1)\n",
    "\n",
    "if os.path.exists(output_dir_bert_mini):\n",
    "    shutil.rmtree(output_dir_bert_mini)\n",
    "\n",
    "metrics, log_history_bert_mini = pretrain(args_train, model_dict_bert_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loss after load_from_source of the same model\n",
    "model_dict_new = model_dict_bert_mini\n",
    "model_graph_new = graph_util.model_dict_to_graph(model_dict_new)\n",
    "model_hash_new = graph_util.hash_graph(*model_graph_new) + '_new'\n",
    "\n",
    "print(f'New model\\'s hash: {model_hash_new}')\n",
    "\n",
    "output_dir_new = f'./models/{model_hash_new}'\n",
    "\n",
    "chosen_neighbor_model = BertModelModular.from_pretrained(output_dir_bert_mini)\n",
    "\n",
    "# Finding the latest checkpoint for BERT-Mini\n",
    "PREFIX_CHECKPOINT_DIR = \"checkpoint\"\n",
    "_re_checkpoint = re.compile(r\"^\" + PREFIX_CHECKPOINT_DIR + r\"\\-(\\d+)$\")\n",
    "\n",
    "content = os.listdir(output_dir_bert_mini)\n",
    "checkpoints = [\n",
    "        path\n",
    "        for path in content\n",
    "        if _re_checkpoint.search(path) is not None and os.path.isdir(os.path.join(output_dir_bert_mini, path))\n",
    "    ]\n",
    "checkpoint_dir = max(checkpoints, key=lambda x: int(_re_checkpoint.search(x).groups()[0]))\n",
    "# print(checkpoint_dir)\n",
    "                \n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "config_new = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "config_new.from_model_dict_hetero(model_dict_new)\n",
    "    \n",
    "model_new = BertModelModular(config_new)\n",
    "model_new.load_model_from_source(chosen_neighbor_model)\n",
    "\n",
    "# Setting up checkpoint for new model\n",
    "if os.path.exists(output_dir_new):\n",
    "    shutil.rmtree(output_dir_new)\n",
    "shutil.copytree(os.path.join(output_dir_bert_mini, checkpoint_dir), os.path.join(output_dir_new, checkpoint_dir))\n",
    "os.remove(os.path.join(output_dir_new, checkpoint_dir, 'optimizer.pt'))\n",
    "# os.remove(os.path.join(output_dir_new, checkpoint_dir, 'scheduler.pt'))\n",
    "model_new.save_pretrained(os.path.join(output_dir_new, checkpoint_dir))\n",
    "\n",
    "args_train = get_training_args(0, 200, 32, output_dir_new, -1)\n",
    "\n",
    "metrics, log_history_bert_mini_new = pretrain(args_train, model_dict_bert_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loss from manual GP\n",
    "model_dict_new = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']]*3 + \\\n",
    "                  [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_wma_64']], \n",
    "        'h': [256]*4, 'f': [[1024]]*4}\n",
    "model_graph_new = graph_util.model_dict_to_graph(model_dict_new)\n",
    "model_hash_new = graph_util.hash_graph(*model_graph_new)\n",
    "\n",
    "print(f'New model\\'s hash: {model_hash_new}')\n",
    "\n",
    "output_dir_new = f'./models/{model_hash_new}'\n",
    "\n",
    "chosen_neighbor_model = BertModelModular.from_pretrained(output_dir_bert_mini)\n",
    "\n",
    "# Finding the latest checkpoint for BERT-Mini\n",
    "PREFIX_CHECKPOINT_DIR = \"checkpoint\"\n",
    "_re_checkpoint = re.compile(r\"^\" + PREFIX_CHECKPOINT_DIR + r\"\\-(\\d+)$\")\n",
    "\n",
    "content = os.listdir(output_dir_bert_mini)\n",
    "checkpoints = [\n",
    "        path\n",
    "        for path in content\n",
    "        if _re_checkpoint.search(path) is not None and os.path.isdir(os.path.join(output_dir_bert_mini, path))\n",
    "    ]\n",
    "checkpoint_dir = max(checkpoints, key=lambda x: int(_re_checkpoint.search(x).groups()[0]))\n",
    "# print(checkpoint_dir)\n",
    "                \n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "config_new = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "config_new.from_model_dict_hetero(model_dict_new)\n",
    "    \n",
    "model_new = BertModelModular(config_new)\n",
    "model_new.load_model_from_source(chosen_neighbor_model)\n",
    "\n",
    "# Setting up checkpoint for new model\n",
    "if os.path.exists(output_dir_new):\n",
    "    shutil.rmtree(output_dir_new)\n",
    "shutil.copytree(os.path.join(output_dir_bert_mini, checkpoint_dir), os.path.join(output_dir_new, checkpoint_dir))\n",
    "os.remove(os.path.join(output_dir_new, checkpoint_dir, 'optimizer.pt'))\n",
    "# os.remove(os.path.join(output_dir_new, checkpoint_dir, 'scheduler.pt'))\n",
    "model_new.save_pretrained(os.path.join(output_dir_new, checkpoint_dir))\n",
    "\n",
    "args_train = get_training_args(0, 200, 32, output_dir_new, -1)\n",
    "\n",
    "metrics, log_history_model_new = pretrain(args_train, model_dict_bert_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss of grown model vs. original model with loss saturated\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot([state['step'] for state in log_history_bert_mini[:-1]], \n",
    "        [state['loss'] for state in log_history_bert_mini[:-1]], label='BERT-Mini')\n",
    "plt.plot([state['step'] for state in log_history_model_new[10:-1]], \n",
    "        [state['loss'] for state in log_history_model_new[10:-1]], label='Grown Model')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Training Steps')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test automatic block-level growth\n",
    "model_dict = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']], \n",
    "                     'h': [256]*4, 'f': [[1024]]*4}\n",
    "\n",
    "ops = ['sa_sdp', 'sa_wma', 'l_dft', 'l_dct', 'c_5', 'c_9', 'c_13']\n",
    "\n",
    "num_samples = 10\n",
    "for sample_idx in range(num_samples):\n",
    "    layer = random.randint(0, model_dict['l']-1)\n",
    "    op = random.sample(ops, 1)[0]\n",
    "    \n",
    "    model_dict_new = deepcopy(model_dict)\n",
    "    \n",
    "    layer_hidden_dim = model_dict_new['o'][layer][0].split('_')[2]\n",
    "    model_dict_new['o'][layer].append(op + '_' +  layer_hidden_dim)\n",
    "    \n",
    "    print(model_dict_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test block-level pruning\n",
    "model_dict = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_wma_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'c_5_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']], \n",
    "                     'h': [256]*4, 'f': [[1024, 1024, 1024]]*4}\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "\n",
    "config = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "config.from_model_dict_hetero(model_dict)\n",
    "\n",
    "model = BertForMaskedLMModular(config)\n",
    "\n",
    "for i in range(model_dict['l']):\n",
    "    attention_head_size = int(model_dict['o'][i][0].split('_')[2])\n",
    "    attention_layer = model.bert.encoder.layer[i].attention.self\n",
    "    wma_count, conv_count = 0, 0\n",
    "    for j in range(len(model_dict['o'][i])):\n",
    "        query_mean = torch.mean(torch.square(\n",
    "            attention_layer.query.weight[j*attention_head_size:(j+1)*attention_head_size])).item()\n",
    "        key_mean = torch.mean(torch.square(\n",
    "            attention_layer.key.weight[j*attention_head_size:(j+1)*attention_head_size])).item()\n",
    "        value_mean = torch.mean(torch.square(\n",
    "            attention_layer.value.weight[j*attention_head_size:(j+1)*attention_head_size])).item()\n",
    "        weights = [query_mean, key_mean, value_mean]\n",
    "        if model_dict['o'][i][j].split('_')[1] == 'wma':\n",
    "            wma_mean = torch.mean(torch.square(\n",
    "                getattr(attention_layer, f'W{wma_count}'))).item()\n",
    "            weights.append(wma_mean)\n",
    "            wma_count += 1\n",
    "        elif model_dict['o'][i][j].split('_')[1].isnumeric():\n",
    "            key_conv_attn_layer_mean = np.mean([torch.mean(torch.square(\n",
    "                getattr(attention_layer, f'key_conv_attn_layer{conv_count}').depthwise.weight)).item(),\n",
    "                                                torch.mean(torch.square(\n",
    "                getattr(attention_layer, f'key_conv_attn_layer{conv_count}').pointwise.weight)).item()])\n",
    "            conv_kernel_layer_mean = torch.mean(torch.square(\n",
    "                getattr(attention_layer, f'conv_kernel_layer{conv_count}').weight)).item()\n",
    "            conv_out_layer_mean = torch.mean(torch.square(\n",
    "                getattr(attention_layer, f'conv_out_layer{conv_count}').weight)).item()\n",
    "            conv_mean = np.mean([key_conv_attn_layer_mean, conv_kernel_layer_mean, conv_out_layer_mean])\n",
    "            weights.append(conv_mean)\n",
    "            conv_count += 1\n",
    "        print(f'Layer {i}, Attention head {j}, mean: {np.mean(weights): 0.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Trainer\n",
    "tokenized_datasets = load_from_disk(f'../../txf_design-space/tokenized_pretraining_dataset')\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "train_dataset = train_dataset.select(range(100))\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.1,\n",
    "    pad_to_multiple_of=None,\n",
    ")\n",
    "\n",
    "training_args = get_training_args(0, 1, 1, './models/test_model/', -1)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The model checkpoint for weights initialization.\"\n",
    "            \"Don't set if you want to train a model from scratch.\"\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    validation_split_percentage: Optional[int] = field(\n",
    "        default=5,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated.\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    mlm_probability: float = field(\n",
    "        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n",
    "    )\n",
    "    line_by_line: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_val_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of validation examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    '''\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, a json or a txt file.\"\n",
    "    '''\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "_, _, training_args = parser.parse_args_into_dataclasses(args_train)\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FFNN gradients\n",
    "if os.path.exists('./models/test_model/'):\n",
    "    shutil.rmtree('./models/test_model/')\n",
    "    \n",
    "args_train = get_training_args(0, 1, 1, './models/test_model/', -1)\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "_, _, args_train = parser.parse_args_into_dataclasses(args_train)\n",
    "set_seed(args_train.seed)\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args_train,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "for step, inputs in enumerate(train_dataloader):\n",
    "    model.train()\n",
    "    inputs = trainer._prepare_inputs(inputs)\n",
    "    loss = trainer.compute_loss(model, inputs)\n",
    "    loss.backward()\n",
    "    break\n",
    "    \n",
    "weight_grad = model.bert.encoder.layer[0].intermediate.sequential[0].weight.grad.cpu().numpy()\n",
    "print(f'weight_grad.shape: {weight_grad.shape}')\n",
    "weight_grad = np.abs(weight_grad)\n",
    "\n",
    "print(f'argmax(weight_grad): {np.unravel_index(np.argmax(weight_grad), weight_grad.shape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test weight gradients for neuron-level growth in FFNN\n",
    "\n",
    "models_dir = '../models'\n",
    "model_hash = '07aaba14d29455a984e2aef6312a8870'\n",
    "\n",
    "if os.path.exists(os.path.join(models_dir, 'test_models', model_hash)):\n",
    "    shutil.rmtree(os.path.join(models_dir, 'test_models', model_hash))\n",
    "shutil.copytree(os.path.join(models_dir, model_hash), os.path.join(models_dir, 'test_models', model_hash))\n",
    "\n",
    "# Get model dictionary\n",
    "model_dict = json.load(open(os.path.join(models_dir, 'test_models', model_hash, 'model_dict.json'), 'r'))\n",
    "\n",
    "# Get training arguments for pre-training\n",
    "def _get_training_args(seed, max_steps, per_gpu_batch_size, output_dir, local_rank):\n",
    "    a = \"--seed {} \\\n",
    "    --do_train \\\n",
    "    --max_seq_length 512 \\\n",
    "    --per_gpu_train_batch_size {} \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --max_steps {} \\\n",
    "    --adam_epsilon 1e-6 \\\n",
    "    --adam_beta2 0.98 \\\n",
    "    --learning_rate 1000e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --warmup_steps 10000 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --output_dir {} \\\n",
    "    --local_rank {} \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 10 \\\n",
    "        \".format(seed, per_gpu_batch_size, max_steps, output_dir, local_rank)\n",
    "    return shlex.split(a)\n",
    "\n",
    "# Instantiate Trainer\n",
    "tokenized_datasets = load_from_disk(f'../../txf_design-space/tokenized_pretraining_dataset')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../../txf_design-space/roberta_tokenizer/')\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "train_dataset = train_dataset.select(range(100))\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.1,\n",
    "    pad_to_multiple_of=None,\n",
    ")\n",
    "\n",
    "# Get arguments for pre-training\n",
    "training_args = _get_training_args(0, 1, 16, os.path.join(models_dir, 'test_models', model_hash), -1)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The model checkpoint for weights initialization.\"\n",
    "            \"Don't set if you want to train a model from scratch.\"\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    validation_split_percentage: Optional[int] = field(\n",
    "        default=5,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated.\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    mlm_probability: float = field(\n",
    "        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n",
    "    )\n",
    "    line_by_line: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_val_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of validation examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "local_parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "_, _, training_args = local_parser.parse_args_into_dataclasses(training_args)\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "# Instantiate model\n",
    "model = BertForMaskedLMModular.from_pretrained(os.path.join(models_dir, 'test_models', model_hash, 'checkpoint-72000'))\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "for step, inputs in enumerate(train_dataloader):\n",
    "    model.train()\n",
    "    inputs = trainer._prepare_inputs(inputs)\n",
    "    loss = trainer.compute_loss(model, inputs)\n",
    "    loss.backward()\n",
    "    break\n",
    "\n",
    "weight_grads = []\n",
    "\n",
    "for i in range(model_dict['l']):\n",
    "    for j in range(len(model_dict['f'][i])):\n",
    "        weight_grad = model.bert.encoder.layer[i].intermediate.sequential[j].weight.grad.cpu().numpy()\n",
    "        weight_grad = np.abs(weight_grad)\n",
    "\n",
    "        weight_grads.append({'layer': i, 'feed_forward_layer': j, 'weight_grad': np.mean(weight_grad)})\n",
    "  \n",
    "print(weight_grads)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test weight values for neuron-level pruning in FFNN\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    plt.plot(np.sort(np.mean(\n",
    "        np.abs(model.bert.encoder.layer[0].intermediate.sequential[0].weight.cpu().numpy()), axis=1)))\n",
    "plt.show()\n",
    "\n",
    "for i in range(model_dict['l']):\n",
    "    per_improvs = []\n",
    "    for j in range(len(model_dict['f'][i])):\n",
    "        with torch.no_grad():\n",
    "            weights_sorted = np.sort(np.mean(\n",
    "                np.abs(model.bert.encoder.layer[i].intermediate.sequential[0].weight.cpu().numpy()), axis=1))\n",
    "\n",
    "        weights_mean = np.mean(weights_sorted)\n",
    "        weights_pruned = np.mean(weights_sorted[16:])\n",
    "        \n",
    "        per_improv = (weights_pruned - weights_mean) / weights_mean\n",
    "        per_improvs.append(per_improv)\n",
    "\n",
    "    print(np.mean(per_improvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pretrain_model.py\n",
    "PREFIX_CHECKPOINT_DIR = \"checkpoint\"\n",
    "bert_hash = '07aaba14d29455a984e2aef6312a8870'\n",
    "\n",
    "models_dir = '../models/'\n",
    "model_dict_bert_base = json.load(open(os.path.join(models_dir, bert_hash, 'model_dict.json'), 'r'))\n",
    "model_dict = deepcopy(model_dict_bert_base)\n",
    "model_dict['o'][0].append('sa_wma_64')\n",
    "print(model_dict)\n",
    "\n",
    "model_graph = graph_util.model_dict_to_graph(model_dict)\n",
    "model_hash = graph_util.hash_graph(*model_graph)\n",
    "\n",
    "chosen_neighbor_path = os.path.join(models_dir, bert_hash)\n",
    "model_path = os.path.join(models_dir, model_hash)\n",
    "\n",
    "chosen_neighbor_model = BertForMaskedLMModular.from_pretrained(chosen_neighbor_path)\n",
    "\n",
    "# Finding the latest checkpoint for chosen neighbor\n",
    "re_checkpoint = re.compile(r\"^\" + PREFIX_CHECKPOINT_DIR + r\"\\-(\\d+)$\")\n",
    "content = os.listdir(chosen_neighbor_path)\n",
    "checkpoints = [\n",
    "        path\n",
    "        for path in content\n",
    "        if re_checkpoint.search(path) is not None and os.path.isdir(os.path.join(chosen_neighbor_path, path))\n",
    "    ]\n",
    "checkpoint_dir = max(checkpoints, key=lambda x: int(re_checkpoint.search(x).groups()[0]))\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "config_new = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "config_new.from_model_dict_hetero(model_dict)\n",
    "\n",
    "# Transfer weights from chosen neighbor to the current model\n",
    "model = BertForMaskedLMModular(config_new, transfer_mode='RP')\n",
    "model.load_model_from_source(chosen_neighbor_model, debug=True)\n",
    "\n",
    "# Setting up checkpoint for the current model\n",
    "if os.path.exists(model_path):\n",
    "    shutil.rmtree(model_path)\n",
    "\n",
    "shutil.copytree(os.path.join(chosen_neighbor_path), os.path.join(model_path))\n",
    "\n",
    "try:\n",
    "    os.remove(os.path.join(model_path, checkpoint_dir, 'optimizer.pt'))\n",
    "    # os.remove(os.path.join(output_dir_new, checkpoint_dir, 'scheduler.pt'))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model.save_pretrained(os.path.join(model_path, checkpoint_dir))\n",
    "\n",
    "# Save model dictionary\n",
    "json.dump(model_dict, open(os.path.join(models_dir, model_hash, 'model_dict.json'), 'w+'))\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Input parameters for pretraining',\n",
    "    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--output_dir',\n",
    "    metavar='',\n",
    "    type=str,\n",
    "    default=model_path,\n",
    "    help='path to save the pretrained model')\n",
    "parser.add_argument('--steps',\n",
    "    metavar='',\n",
    "    type=int,\n",
    "    default=20,\n",
    "    help='number of steps to pre-train beyond latest checkpoint')\n",
    "parser.add_argument('--local_rank',\n",
    "    metavar='',\n",
    "    type=int,\n",
    "    help='rank of the process during distributed training',\n",
    "    default=-1)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# pretrain_model.main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model dictionary for BERT-Base\n",
    "model_dict_bert_base = {'l': 12, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64']], 'h': [768, 768, 768, 768, 768, 768, \\\n",
    "                                                                        768, 768, 768, 768, 768, 768], \\\n",
    "                                                                    'f': [[3072], [3072], [3072], \\\n",
    "                                                                        [3072], [3072], [3072], [3072], [3072], \\\n",
    "                                                                        [3072], [3072], [3072], [3072]]}\n",
    "json.dump(model_dict_bert_base, open(os.path.join(models_dir, bert_hash, 'model_dict.json'), 'w+'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txf_design-space [~/.conda/envs/txf_design-space/]",
   "language": "python",
   "name": "conda_txf_design-space"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
