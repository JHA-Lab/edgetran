{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../../txf_design-space/embeddings/utils')\n",
    "sys.path.append('../../txf_design-space/flexibert')\n",
    "import graph_util\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import shlex\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from roberta_pretraining import pretrain\n",
    "import shutil\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, interleave_datasets, load_from_disk\n",
    "from transformers.models.bert.modeling_modular_bert import BertModelModular, BertForMaskedLMModular\n",
    "from transformers import BertModel\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_MASKED_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.attention.self.distance_embedding.weight', 'roberta.encoder.layer.1.attention.self.distance_embedding.weight', 'roberta.encoder.layer.2.attention.self.distance_embedding.weight', 'roberta.encoder.layer.3.attention.self.distance_embedding.weight', 'roberta.encoder.layer.4.attention.self.distance_embedding.weight', 'roberta.encoder.layer.5.attention.self.distance_embedding.weight', 'roberta.encoder.layer.6.attention.self.distance_embedding.weight', 'roberta.encoder.layer.7.attention.self.distance_embedding.weight', 'roberta.encoder.layer.8.attention.self.distance_embedding.weight', 'roberta.encoder.layer.9.attention.self.distance_embedding.weight', 'roberta.encoder.layer.10.attention.self.distance_embedding.weight', 'roberta.encoder.layer.11.attention.self.distance_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash of BERT-Base: 07aaba14d29455a984e2aef6312a8870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./models/07aaba14d29455a984e2aef6312a8870/tokenizer_config.json',\n",
       " './models/07aaba14d29455a984e2aef6312a8870/special_tokens_map.json',\n",
       " './models/07aaba14d29455a984e2aef6312a8870/vocab.json',\n",
       " './models/07aaba14d29455a984e2aef6312a8870/merges.txt',\n",
       " './models/07aaba14d29455a984e2aef6312a8870/added_tokens.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transfer weights of BERT-Base to heterogeneous counterpart\n",
    "roberta_base = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "model_dict_hetero = {'l': 12, 'o': [['sa_sdp_64']*12]*12, 'h': [768]*12, 'f': [[3072]]*12}\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "config = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "\n",
    "config.from_model_dict_hetero(model_dict_hetero)\n",
    "\n",
    "model = BertModelModular(config)\n",
    "\n",
    "model_state_dict = model.state_dict()\n",
    "model_state_dict.update(roberta_base.state_dict())\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "model_graph = graph_util.model_dict_to_graph(model_dict_hetero)\n",
    "model_hash = graph_util.hash_graph(*model_graph)\n",
    "output_dir = '../models/'+model_hash+'/'\n",
    "\n",
    "print(f'Hash of BERT-Base: {model_hash}')\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model dict for BERT-Mini like model\n",
    "model_dict = {'l': 4, 'o': ['sa']*4, 'h': [256]*4, 'n': [4]*4, 'f': [[1024]]*4, 'p': ['sdp']*4}\n",
    "model_dict_hetero = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'c_5_64', 'sa_sdp_64']]*4, \n",
    "                     'h': [256]*4, 'f': [[1024]]*4}\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "\n",
    "config = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "# config.from_model_dict(model_dict)\n",
    "\n",
    "config.from_model_dict_hetero(model_dict_hetero)\n",
    "\n",
    "model = BertForMaskedLMModular(config)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking layer 0...\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: c_13_64\n",
      "\tTransfering attention head 3: sa_wma_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 1...\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: c_13_64\n",
      "\tTransfering attention head 3: sa_wma_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 2...\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: c_13_64\n",
      "\tTransfering attention head 3: sa_wma_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 3...\n",
      "\tTransfering attention head 0: sa_sdp_32\n",
      "\tTransfering attention head 1: sa_sdp_32\n",
      "\tTransfering attention head 2: c_5_32\n",
      "\tTransfering attention head 3: sa_wma_32\n",
      "\tTransfering feed-forward layer 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test load_from_source()\n",
    "# model_dict_hetero2 = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']]*3 + \\\n",
    "#                                    [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_wma_64']], \n",
    "#                       'h': [256]*4, 'f': [[1024, 1024]]*4}\n",
    "model_dict_hetero2 = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'c_13_64', 'sa_wma_64']]*3 + \\\n",
    "                                    [['sa_sdp_32', 'sa_sdp_32', 'c_5_32', 'sa_wma_32']], \n",
    "                     'h': [128, 256, 256, 512], 'f': [[2048, 256]]*4}\n",
    "# model_dict_hetero2 = {'l': 4, 'o': [['sa_sdp_32', 'sa_sdp_32', 'c_13_32', 'sa_sdp_32']]*4, \n",
    "#                      'h': [256]*4, 'f': [[1024]]*4}\n",
    "\n",
    "config2 = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "config2.from_model_dict_hetero(model_dict_hetero2)\n",
    "\n",
    "model2 = BertForMaskedLMModular(config2, transfer_mode='OD')\n",
    "\n",
    "# print(model2)\n",
    "\n",
    "model2.bert.load_model_from_source(model.bert, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.encoder.layer[0].attention.self.conv_kernel_layer0.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test RP\n",
    "from sklearn import random_projection\n",
    "\n",
    "matrix = torch.rand(1000, 1000)\n",
    "matrix_numpy = matrix.cpu().numpy()\n",
    "\n",
    "rp = random_projection.GaussianRandomProjection(128)\n",
    "matrix_numpy_new = rp.fit_transform(matrix_numpy)\n",
    "\n",
    "matrix_numpy_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RP with torch\n",
    "import torch.nn as nn\n",
    "from torch.tensor import Tensor\n",
    "\n",
    "model.bert.encoder.layer[0].attention.self.query.weight = \\\n",
    "    nn.parameter.Parameter(\n",
    "        torch.from_numpy(\n",
    "            rp.fit_transform(model.bert.encoder.layer[0].attention.self.query.weight.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), ['input', 'sa_h128_p-sdp', 'sa_h128_p-sdp', 'add_norm', 'f512', 'f512', 'add_norm', 'sa_h128_p-sdp', 'sa_h128_p-sdp', 'add_norm', 'f512', 'f512', 'add_norm', 'output'])\n"
     ]
    }
   ],
   "source": [
    "# Test graph generation from model dict\n",
    "model_dict = {'l': 2, 'o': ['sa']*2, 'h': [128]*2, 'n': [2]*2, 'f': [[512, 512]]*2, 'p': ['sdp']*2}\n",
    "\n",
    "model_graph = graph_util.model_dict_to_graph(model_dict)\n",
    "\n",
    "print(model_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50ef6167847c070825f29805f4e9cd35'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test hashing for model_dict_hetero\n",
    "model_dict_hetero = {'l': 4, 'o': [['l_dft_64', 'sa_wma_64', 'sa_wma_64', 'c_9_64']]*4, \n",
    "                     'h': [256]*4, 'n': [4]*4, 'f': [[1024]]*4}\n",
    "\n",
    "model_graph = graph_util.model_dict_to_graph(model_dict_hetero)\n",
    "\n",
    "graph_util.hash_graph(*model_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_args(seed, max_steps, per_gpu_batch_size, output_dir, local_rank):\n",
    "    a = \"--seed {} \\\n",
    "    --do_train \\\n",
    "    --max_seq_length 512 \\\n",
    "    --per_gpu_train_batch_size {} \\\n",
    "    --max_steps {} \\\n",
    "    --adam_epsilon 1e-6 \\\n",
    "    --adam_beta2 0.98 \\\n",
    "    --learning_rate 1000e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --warmup_steps 10000 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --output_dir {} \\\n",
    "    --local_rank {} \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 10 \\\n",
    "        \".format(seed, per_gpu_batch_size, max_steps, output_dir, local_rank)\n",
    "    return shlex.split(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:658] 2022-02-04 19:02:08,754 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:581] 2022-02-04 19:02:08,755 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-Mini's model hash: 7f3d35e33a37f8d6a55d3d3ee339d7ba\n",
      "02/04/2022 19:02:08 - WARNING - roberta_pretraining -   Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: False\n",
      "02/04/2022 19:02:08 - INFO - roberta_pretraining -   Training/evaluation parameters TrainingArguments(output_dir=./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/, overwrite_output_dir=False, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, eval_accumulation_steps=None, learning_rate=0.1, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.98, adam_epsilon=1e-06, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=100, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=10000, logging_dir=runs/Feb04_19-02-08_della-i14g7, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=10, save_strategy=IntervalStrategy.STEPS, save_steps=10, save_total_limit=2, no_cuda=False, seed=0, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name=./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=2, mp_parameters=)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1647] 2022-02-04 19:02:08,758 >> Didn't find file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1647] 2022-02-04 19:02:08,758 >> Didn't find file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:02:08,759 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:02:08,759 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:02:08,759 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:02:08,759 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:02:08,760 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:02:08,760 >> loading file None\n",
      "[INFO|trainer.py:375] 2022-02-04 19:02:09,454 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:488] 2022-02-04 19:02:09,455 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLMModular.forward` and have been ignored: special_tokens_mask.\n",
      "[WARNING|training_args.py:633] 2022-02-04 19:02:09,456 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:633] 2022-02-04 19:02:09,458 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:1011] 2022-02-04 19:02:09,458 >> ***** Running training *****\n",
      "[INFO|trainer.py:1012] 2022-02-04 19:02:09,458 >>   Num examples = 3149357\n",
      "[INFO|trainer.py:1013] 2022-02-04 19:02:09,459 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1014] 2022-02-04 19:02:09,459 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1015] 2022-02-04 19:02:09,459 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "[INFO|trainer.py:1016] 2022-02-04 19:02:09,459 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1017] 2022-02-04 19:02:09,460 >>   Total optimization steps = 100\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:10, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10.757500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>10.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>9.872200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>9.194400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.476200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.899800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>7.678200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>7.596300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>7.589700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>7.532600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1667] 2022-02-04 19:02:28,155 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-10\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:02:28,157 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-10/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:02:28,275 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-10/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:02:28,276 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-10/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:02:28,277 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-10/special_tokens_map.json\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:02:47,442 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-20\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:02:47,443 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-20/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:02:47,561 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-20/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:02:47,562 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-20/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:02:47,563 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-20/special_tokens_map.json\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:03:07,175 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-30\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:03:07,176 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-30/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:03:07,296 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-30/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:03:07,297 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-30/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:03:07,298 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-30/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:03:07,676 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-10] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:03:26,611 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-40\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:03:26,612 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-40/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:03:26,746 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-40/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:03:26,747 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-40/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:03:26,747 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-40/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:03:27,013 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-20] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:03:45,541 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-50\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:03:45,542 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-50/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:03:45,661 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-50/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:03:45,662 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-50/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:03:45,663 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-50/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:03:46,028 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-30] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:04:04,775 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-60\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:04:04,776 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-60/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:04:04,900 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-60/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:04:04,901 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-60/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:04:04,902 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-60/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:04:05,278 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-40] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:04:23,525 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-70\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:04:23,526 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-70/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:04:23,645 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-70/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:04:23,646 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-70/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:04:23,646 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-70/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:04:23,994 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-50] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1667] 2022-02-04 19:04:43,336 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-80\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:04:43,338 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-80/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:04:43,457 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-80/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:04:43,458 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-80/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:04:43,458 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-80/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:04:43,834 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-60] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:05:02,702 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-90\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:05:02,703 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-90/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:05:02,826 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-90/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:05:02,828 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-90/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:05:02,828 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-90/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:05:03,102 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-70] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:05:21,577 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-100\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:05:21,578 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-100/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:05:21,694 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-100/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:05:21,695 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:05:21,696 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-100/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:05:22,066 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba/checkpoint-80] due to args.save_total_limit\n",
      "[INFO|trainer.py:1198] 2022-02-04 19:05:22,091 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:05:22,093 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:05:22,094 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:05:22,211 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:05:22,212 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:05:22,213 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:722] 2022-02-04 19:05:22,316 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:727] 2022-02-04 19:05:22,316 >>   epoch                    =       0.01\n",
      "[INFO|trainer_pt_utils.py:727] 2022-02-04 19:05:22,317 >>   total_flos               =  2153706GF\n",
      "[INFO|trainer_pt_utils.py:727] 2022-02-04 19:05:22,317 >>   train_runtime            = 0:03:12.63\n",
      "[INFO|trainer_pt_utils.py:727] 2022-02-04 19:05:22,317 >>   train_samples            =    3149357\n",
      "[INFO|trainer_pt_utils.py:727] 2022-02-04 19:05:22,317 >>   train_samples_per_second =      0.519\n"
     ]
    }
   ],
   "source": [
    "# Test loss from manual GP\n",
    "model_dict_bert_mini = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']]*4, \n",
    "                     'h': [256]*4, 'f': [[1024]]*4}\n",
    "model_graph_bert_mini = graph_util.model_dict_to_graph(model_dict_bert_mini)\n",
    "model_hash_bert_mini = graph_util.hash_graph(*model_graph_bert_mini)\n",
    "\n",
    "print(f'BERT-Mini\\'s model hash: {model_hash_bert_mini}')\n",
    "\n",
    "output_dir_bert_mini = f'./models/{model_hash_bert_mini}/'\n",
    "\n",
    "args_train = get_training_args(0, 100, 32, output_dir_bert_mini, -1)\n",
    "\n",
    "if os.path.exists(output_dir_bert_mini):\n",
    "    shutil.rmtree(output_dir_bert_mini)\n",
    "\n",
    "metrics, log_history_bert_mini = pretrain(args_train, model_dict_bert_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:488] 2022-02-04 19:14:15,869 >> loading configuration file ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/config.json\n",
      "[INFO|configuration_utils.py:526] 2022-02-04 19:14:15,870 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLMModular\"\n",
      "  ],\n",
      "  \"attention_heads_list\": [\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ]\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"conv_kernel_size\": 9,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_dim_list\": [\n",
      "    [\n",
      "      1024\n",
      "    ],\n",
      "    [\n",
      "      1024\n",
      "    ],\n",
      "    [\n",
      "      1024\n",
      "    ],\n",
      "    [\n",
      "      1024\n",
      "    ]\n",
      "  ],\n",
      "  \"from_model_dict_hetero\": true,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"head_ratio\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dim_list\": [\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256\n",
      "  ],\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_groups\": 1,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"relative_key\",\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1067] 2022-02-04 19:14:15,870 >> loading weights file ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model's hash: 7f3d35e33a37f8d6a55d3d3ee339d7ba_new\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1198] 2022-02-04 19:14:16,200 >> All model checkpoint weights were used when initializing BertModelModular.\n",
      "\n",
      "[WARNING|modeling_utils.py:1200] 2022-02-04 19:14:16,200 >> Some weights of BertModelModular were not initialized from the model checkpoint at ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|tokenization_utils_base.py:1647] 2022-02-04 19:14:16,202 >> Didn't find file ../txf_design-space/roberta_tokenizer/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1647] 2022-02-04 19:14:16,203 >> Didn't find file ../txf_design-space/roberta_tokenizer/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:14:16,203 >> loading file ../txf_design-space/roberta_tokenizer/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:14:16,203 >> loading file ../txf_design-space/roberta_tokenizer/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:14:16,203 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:14:16,204 >> loading file ../txf_design-space/roberta_tokenizer/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:14:16,204 >> loading file ../txf_design-space/roberta_tokenizer/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:14:16,204 >> loading file None\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:14:16,780 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-100/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:14:16,845 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-100/pytorch_model.bin\n",
      "[INFO|training_args.py:658] 2022-02-04 19:14:16,851 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:581] 2022-02-04 19:14:16,852 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/04/2022 19:14:16 - INFO - roberta_pretraining -   Checkpoint detected, resuming training at ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-100. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
      "02/04/2022 19:14:16 - WARNING - roberta_pretraining -   Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: False\n",
      "02/04/2022 19:14:16 - INFO - roberta_pretraining -   Training/evaluation parameters TrainingArguments(output_dir=./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new, overwrite_output_dir=False, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, eval_accumulation_steps=None, learning_rate=0.1, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.98, adam_epsilon=1e-06, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=200, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=10000, logging_dir=runs/Feb04_19-14-16_della-i14g7, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=10, save_strategy=IntervalStrategy.STEPS, save_steps=10, save_total_limit=2, no_cuda=False, seed=0, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name=./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=2, mp_parameters=)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1647] 2022-02-04 19:14:16,856 >> Didn't find file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1647] 2022-02-04 19:14:16,856 >> Didn't find file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:14:16,857 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:14:16,857 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:14:16,857 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:14:16,857 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:14:16,858 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:14:16,858 >> loading file None\n",
      "[INFO|trainer.py:375] 2022-02-04 19:14:17,556 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:488] 2022-02-04 19:14:17,556 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLMModular.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:919] 2022-02-04 19:14:17,557 >> Loading model from ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-100).\n",
      "[INFO|configuration_utils.py:488] 2022-02-04 19:14:17,557 >> loading configuration file ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-100/config.json\n",
      "[INFO|configuration_utils.py:526] 2022-02-04 19:14:17,558 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModelModular\"\n",
      "  ],\n",
      "  \"attention_heads_list\": [\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ]\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"conv_kernel_size\": 9,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_dim_list\": [\n",
      "    [\n",
      "      1024\n",
      "    ],\n",
      "    [\n",
      "      1024\n",
      "    ],\n",
      "    [\n",
      "      1024\n",
      "    ],\n",
      "    [\n",
      "      1024\n",
      "    ]\n",
      "  ],\n",
      "  \"from_model_dict_hetero\": true,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"head_ratio\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dim_list\": [\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256\n",
      "  ],\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_groups\": 1,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"relative_key\",\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1067] 2022-02-04 19:14:17,559 >> loading weights file ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-100/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1198] 2022-02-04 19:14:18,139 >> All model checkpoint weights were used when initializing BertForMaskedLMModular.\n",
      "\n",
      "[WARNING|modeling_utils.py:1200] 2022-02-04 19:14:18,139 >> Some weights of BertForMaskedLMModular were not initialized from the model checkpoint at ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-100 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[WARNING|training_args.py:633] 2022-02-04 19:14:18,154 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:633] 2022-02-04 19:14:18,156 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:1011] 2022-02-04 19:14:18,156 >> ***** Running training *****\n",
      "[INFO|trainer.py:1012] 2022-02-04 19:14:18,157 >>   Num examples = 3149357\n",
      "[INFO|trainer.py:1013] 2022-02-04 19:14:18,157 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1014] 2022-02-04 19:14:18,157 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1015] 2022-02-04 19:14:18,157 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "[INFO|trainer.py:1016] 2022-02-04 19:14:18,158 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1017] 2022-02-04 19:14:18,158 >>   Total optimization steps = 200\n",
      "[INFO|trainer.py:1036] 2022-02-04 19:14:18,159 >>   Continuing training from checkpoint, will skip to saved global_step\n",
      "[INFO|trainer.py:1037] 2022-02-04 19:14:18,159 >>   Continuing training from epoch 0\n",
      "[INFO|trainer.py:1038] 2022-02-04 19:14:18,160 >>   Continuing training from global step 100\n",
      "[INFO|trainer.py:1040] 2022-02-04 19:14:18,160 >>   Will skip the first 0 epochs then the first 400 batches in the first epoch.\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 03:11, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>10.428500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>10.142800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>9.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>9.041100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>8.362800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>7.807600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>7.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>7.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>7.435300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>7.398500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1667] 2022-02-04 19:14:51,702 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-110\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:14:51,703 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-110/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:14:51,824 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-110/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:14:51,825 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-110/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:14:51,826 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-110/special_tokens_map.json\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:15:10,668 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-120\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:15:10,669 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-120/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:15:10,805 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-120/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:15:10,806 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-120/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:15:10,807 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-120/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:15:11,229 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-100] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:15:29,807 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-130\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:15:29,808 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-130/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:15:29,931 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-130/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:15:29,932 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-130/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:15:29,932 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-130/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:15:30,324 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-110] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:15:49,297 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-140\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:15:49,298 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-140/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:15:49,422 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-140/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:15:49,423 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-140/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:15:49,424 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-140/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:15:49,743 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-120] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:16:09,244 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-150\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:16:09,245 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-150/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:16:09,370 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-150/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:16:09,371 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-150/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:16:09,372 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-150/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:16:09,691 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-130] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:16:28,368 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-160\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:16:28,369 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-160/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:16:28,483 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-160/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:16:28,484 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-160/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:16:28,484 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-160/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:16:28,858 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-140] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:16:47,975 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-170\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:16:47,977 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-170/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:16:48,097 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-170/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:16:48,099 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-170/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:16:48,099 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-170/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1734] 2022-02-04 19:16:48,496 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-150] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:17:07,544 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-180\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:17:07,545 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-180/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:17:07,672 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-180/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:17:07,673 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-180/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:17:07,674 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-180/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:17:08,129 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-160] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:17:26,981 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-190\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:17:26,982 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-190/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:17:27,095 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-190/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:17:27,096 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-190/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:17:27,096 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-190/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:17:27,376 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-170] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:17:46,409 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-200\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:17:46,410 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-200/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:17:46,523 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-200/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:17:46,524 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:17:46,525 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-200/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:17:46,913 >> Deleting older checkpoint [models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/checkpoint-180] due to args.save_total_limit\n",
      "[INFO|trainer.py:1198] 2022-02-04 19:17:46,939 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:17:46,941 >> Saving model checkpoint to ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:17:46,942 >> Configuration saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:17:47,057 >> Model weights saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:17:47,059 >> tokenizer config file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:17:47,059 >> Special tokens file saved in ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba_new/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:722] 2022-02-04 19:17:47,171 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:727] 2022-02-04 19:17:47,172 >>   epoch                    =       0.02\n",
      "[INFO|trainer_pt_utils.py:727] 2022-02-04 19:17:47,172 >>   total_flos               =  4307413GF\n",
      "[INFO|trainer_pt_utils.py:727] 2022-02-04 19:17:47,172 >>   train_runtime            = 0:03:28.78\n",
      "[INFO|trainer_pt_utils.py:727] 2022-02-04 19:17:47,172 >>   train_samples            =    3149357\n",
      "[INFO|trainer_pt_utils.py:727] 2022-02-04 19:17:47,173 >>   train_samples_per_second =      0.958\n"
     ]
    }
   ],
   "source": [
    "# Test loss after load_from_source of the same model\n",
    "model_dict_new = model_dict_bert_mini\n",
    "model_graph_new = graph_util.model_dict_to_graph(model_dict_new)\n",
    "model_hash_new = graph_util.hash_graph(*model_graph_new) + '_new'\n",
    "\n",
    "print(f'New model\\'s hash: {model_hash_new}')\n",
    "\n",
    "output_dir_new = f'./models/{model_hash_new}'\n",
    "\n",
    "chosen_neighbor_model = BertModelModular.from_pretrained(output_dir_bert_mini)\n",
    "\n",
    "# Finding the latest checkpoint for BERT-Mini\n",
    "PREFIX_CHECKPOINT_DIR = \"checkpoint\"\n",
    "_re_checkpoint = re.compile(r\"^\" + PREFIX_CHECKPOINT_DIR + r\"\\-(\\d+)$\")\n",
    "\n",
    "content = os.listdir(output_dir_bert_mini)\n",
    "checkpoints = [\n",
    "        path\n",
    "        for path in content\n",
    "        if _re_checkpoint.search(path) is not None and os.path.isdir(os.path.join(output_dir_bert_mini, path))\n",
    "    ]\n",
    "checkpoint_dir = max(checkpoints, key=lambda x: int(_re_checkpoint.search(x).groups()[0]))\n",
    "# print(checkpoint_dir)\n",
    "                \n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "config_new = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "config_new.from_model_dict_hetero(model_dict_new)\n",
    "    \n",
    "model_new = BertModelModular(config_new)\n",
    "model_new.load_model_from_source(chosen_neighbor_model)\n",
    "\n",
    "# Setting up checkpoint for new model\n",
    "if os.path.exists(output_dir_new):\n",
    "    shutil.rmtree(output_dir_new)\n",
    "shutil.copytree(os.path.join(output_dir_bert_mini, checkpoint_dir), os.path.join(output_dir_new, checkpoint_dir))\n",
    "os.remove(os.path.join(output_dir_new, checkpoint_dir, 'optimizer.pt'))\n",
    "# os.remove(os.path.join(output_dir_new, checkpoint_dir, 'scheduler.pt'))\n",
    "model_new.save_pretrained(os.path.join(output_dir_new, checkpoint_dir))\n",
    "\n",
    "args_train = get_training_args(0, 200, 32, output_dir_new, -1)\n",
    "\n",
    "metrics, log_history_bert_mini_new = pretrain(args_train, model_dict_bert_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:488] 2022-02-04 19:05:26,196 >> loading configuration file ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/config.json\n",
      "[INFO|configuration_utils.py:526] 2022-02-04 19:05:26,197 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLMModular\"\n",
      "  ],\n",
      "  \"attention_heads_list\": [\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ]\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"conv_kernel_size\": 9,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_dim_list\": [\n",
      "    [\n",
      "      1024\n",
      "    ],\n",
      "    [\n",
      "      1024\n",
      "    ],\n",
      "    [\n",
      "      1024\n",
      "    ],\n",
      "    [\n",
      "      1024\n",
      "    ]\n",
      "  ],\n",
      "  \"from_model_dict_hetero\": true,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"head_ratio\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dim_list\": [\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256\n",
      "  ],\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_groups\": 1,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"relative_key\",\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1067] 2022-02-04 19:05:26,197 >> loading weights file ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model's hash: 03ac63b9cb63c3d5686db66abd81b991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1198] 2022-02-04 19:05:26,508 >> All model checkpoint weights were used when initializing BertModelModular.\n",
      "\n",
      "[WARNING|modeling_utils.py:1200] 2022-02-04 19:05:26,509 >> Some weights of BertModelModular were not initialized from the model checkpoint at ./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[INFO|tokenization_utils_base.py:1647] 2022-02-04 19:05:26,511 >> Didn't find file ../txf_design-space/roberta_tokenizer/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1647] 2022-02-04 19:05:26,511 >> Didn't find file ../txf_design-space/roberta_tokenizer/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:05:26,511 >> loading file ../txf_design-space/roberta_tokenizer/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:05:26,512 >> loading file ../txf_design-space/roberta_tokenizer/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:05:26,512 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:05:26,512 >> loading file ../txf_design-space/roberta_tokenizer/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:05:26,512 >> loading file ../txf_design-space/roberta_tokenizer/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:05:26,513 >> loading file None\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:05:27,320 >> Configuration saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-100/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:05:27,382 >> Model weights saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-100/pytorch_model.bin\n",
      "[INFO|training_args.py:658] 2022-02-04 19:05:27,388 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:581] 2022-02-04 19:05:27,389 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/04/2022 19:05:27 - INFO - roberta_pretraining -   Checkpoint detected, resuming training at ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-100. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
      "02/04/2022 19:05:27 - WARNING - roberta_pretraining -   Process rank: -1, device: cuda:0, n_gpu: 2distributed training: False, 16-bits training: False\n",
      "02/04/2022 19:05:27 - INFO - roberta_pretraining -   Training/evaluation parameters TrainingArguments(output_dir=./models/03ac63b9cb63c3d5686db66abd81b991, overwrite_output_dir=False, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, eval_accumulation_steps=None, learning_rate=0.1, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.98, adam_epsilon=1e-06, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=200, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=10000, logging_dir=runs/Feb04_19-05-27_della-i14g7, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=10, save_strategy=IntervalStrategy.STEPS, save_steps=10, save_total_limit=2, no_cuda=False, seed=0, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name=./models/03ac63b9cb63c3d5686db66abd81b991, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=2, mp_parameters=)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1647] 2022-02-04 19:05:27,392 >> Didn't find file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1647] 2022-02-04 19:05:27,393 >> Didn't find file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:05:27,393 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:05:27,393 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:05:27,394 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:05:27,394 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:05:27,394 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-04 19:05:27,394 >> loading file None\n",
      "[INFO|trainer.py:375] 2022-02-04 19:05:28,084 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:488] 2022-02-04 19:05:28,085 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLMModular.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:919] 2022-02-04 19:05:28,085 >> Loading model from ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-100).\n",
      "[INFO|configuration_utils.py:488] 2022-02-04 19:05:28,086 >> loading configuration file ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-100/config.json\n",
      "[INFO|configuration_utils.py:526] 2022-02-04 19:05:28,087 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModelModular\"\n",
      "  ],\n",
      "  \"attention_heads_list\": [\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_wma_64\"\n",
      "    ]\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"conv_kernel_size\": 9,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_dim_list\": [\n",
      "    [\n",
      "      1024\n",
      "    ],\n",
      "    [\n",
      "      1024\n",
      "    ],\n",
      "    [\n",
      "      1024\n",
      "    ],\n",
      "    [\n",
      "      1024\n",
      "    ]\n",
      "  ],\n",
      "  \"from_model_dict_hetero\": true,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"head_ratio\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dim_list\": [\n",
      "    256,\n",
      "    256,\n",
      "    256,\n",
      "    256\n",
      "  ],\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_groups\": 1,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"relative_key\",\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1067] 2022-02-04 19:05:28,087 >> loading weights file ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-100/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1198] 2022-02-04 19:05:28,662 >> All model checkpoint weights were used when initializing BertForMaskedLMModular.\n",
      "\n",
      "[WARNING|modeling_utils.py:1200] 2022-02-04 19:05:28,662 >> Some weights of BertForMaskedLMModular were not initialized from the model checkpoint at ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-100 and are newly initialized: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[WARNING|training_args.py:633] 2022-02-04 19:05:28,677 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:633] 2022-02-04 19:05:28,678 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:1011] 2022-02-04 19:05:28,679 >> ***** Running training *****\n",
      "[INFO|trainer.py:1012] 2022-02-04 19:05:28,679 >>   Num examples = 3149357\n",
      "[INFO|trainer.py:1013] 2022-02-04 19:05:28,679 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1014] 2022-02-04 19:05:28,680 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1015] 2022-02-04 19:05:28,680 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "[INFO|trainer.py:1016] 2022-02-04 19:05:28,680 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1017] 2022-02-04 19:05:28,681 >>   Total optimization steps = 200\n",
      "[INFO|trainer.py:1036] 2022-02-04 19:05:28,681 >>   Continuing training from checkpoint, will skip to saved global_step\n",
      "[INFO|trainer.py:1037] 2022-02-04 19:05:28,681 >>   Continuing training from epoch 0\n",
      "[INFO|trainer.py:1038] 2022-02-04 19:05:28,682 >>   Continuing training from global step 100\n",
      "[INFO|trainer.py:1040] 2022-02-04 19:05:28,682 >>   Will skip the first 0 epochs then the first 400 batches in the first epoch.\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 03:16, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>10.786500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>10.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>9.902700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>9.216100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>8.477100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>7.906400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>7.662600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>7.599600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>7.549600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>7.492200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1667] 2022-02-04 19:06:03,262 >> Saving model checkpoint to ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-110\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:06:03,264 >> Configuration saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-110/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:06:03,381 >> Model weights saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-110/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:06:03,383 >> tokenizer config file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-110/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:06:03,383 >> Special tokens file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-110/special_tokens_map.json\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:06:23,236 >> Saving model checkpoint to ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-120\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:06:23,238 >> Configuration saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-120/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:06:23,351 >> Model weights saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-120/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:06:23,352 >> tokenizer config file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-120/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:06:23,352 >> Special tokens file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-120/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:06:23,706 >> Deleting older checkpoint [models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-100] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:06:43,080 >> Saving model checkpoint to ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-130\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:06:43,082 >> Configuration saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-130/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:06:43,196 >> Model weights saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-130/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:06:43,197 >> tokenizer config file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-130/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:06:43,198 >> Special tokens file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-130/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:06:43,483 >> Deleting older checkpoint [models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-110] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:07:02,671 >> Saving model checkpoint to ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-140\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:07:02,672 >> Configuration saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-140/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:07:02,787 >> Model weights saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-140/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:07:02,788 >> tokenizer config file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-140/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:07:02,788 >> Special tokens file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-140/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:07:03,077 >> Deleting older checkpoint [models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-120] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:07:22,580 >> Saving model checkpoint to ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-150\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:07:22,581 >> Configuration saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-150/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:07:22,693 >> Model weights saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-150/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:07:22,694 >> tokenizer config file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-150/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:07:22,695 >> Special tokens file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-150/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:07:22,967 >> Deleting older checkpoint [models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-130] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:07:42,600 >> Saving model checkpoint to ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-160\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:07:42,601 >> Configuration saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-160/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:07:42,715 >> Model weights saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-160/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:07:42,716 >> tokenizer config file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-160/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:07:42,716 >> Special tokens file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-160/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:07:42,994 >> Deleting older checkpoint [models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-140] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:08:02,442 >> Saving model checkpoint to ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-170\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:08:02,443 >> Configuration saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-170/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:08:02,555 >> Model weights saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-170/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:08:02,557 >> tokenizer config file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-170/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:08:02,557 >> Special tokens file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-170/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:08:02,840 >> Deleting older checkpoint [models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-150] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:08:22,297 >> Saving model checkpoint to ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-180\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:08:22,298 >> Configuration saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-180/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:08:22,411 >> Model weights saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-180/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:08:22,412 >> tokenizer config file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-180/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:08:22,413 >> Special tokens file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-180/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:08:22,687 >> Deleting older checkpoint [models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-160] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:08:42,603 >> Saving model checkpoint to ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-190\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:08:42,604 >> Configuration saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-190/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:08:42,716 >> Model weights saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-190/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:08:42,717 >> tokenizer config file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-190/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:08:42,717 >> Special tokens file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-190/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:08:42,988 >> Deleting older checkpoint [models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-170] due to args.save_total_limit\n",
      "/home/stuli/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:65: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:09:02,121 >> Saving model checkpoint to ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-200\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:09:02,122 >> Configuration saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-200/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:09:02,241 >> Model weights saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-200/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:09:02,242 >> tokenizer config file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:09:02,242 >> Special tokens file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-200/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-04 19:09:02,595 >> Deleting older checkpoint [models/03ac63b9cb63c3d5686db66abd81b991/checkpoint-180] due to args.save_total_limit\n",
      "[INFO|trainer.py:1198] 2022-02-04 19:09:02,619 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|trainer.py:1667] 2022-02-04 19:09:02,621 >> Saving model checkpoint to ./models/03ac63b9cb63c3d5686db66abd81b991\n",
      "[INFO|configuration_utils.py:329] 2022-02-04 19:09:02,622 >> Configuration saved in ./models/03ac63b9cb63c3d5686db66abd81b991/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-04 19:09:02,735 >> Model weights saved in ./models/03ac63b9cb63c3d5686db66abd81b991/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-04 19:09:02,736 >> tokenizer config file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-04 19:09:02,736 >> Special tokens file saved in ./models/03ac63b9cb63c3d5686db66abd81b991/special_tokens_map.json\n",
      "[INFO|trainer_pt_utils.py:722] 2022-02-04 19:09:02,840 >> ***** train metrics *****\n",
      "[INFO|trainer_pt_utils.py:727] 2022-02-04 19:09:02,841 >>   epoch                    =       0.02\n",
      "[INFO|trainer_pt_utils.py:727] 2022-02-04 19:09:02,841 >>   total_flos               =  4312527GF\n",
      "[INFO|trainer_pt_utils.py:727] 2022-02-04 19:09:02,841 >>   train_runtime            = 0:03:33.93\n",
      "[INFO|trainer_pt_utils.py:727] 2022-02-04 19:09:02,842 >>   train_samples            =    3149357\n",
      "[INFO|trainer_pt_utils.py:727] 2022-02-04 19:09:02,842 >>   train_samples_per_second =      0.935\n"
     ]
    }
   ],
   "source": [
    "# Test loss from manual GP\n",
    "model_dict_new = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']]*3 + \\\n",
    "                  [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_wma_64']], \n",
    "        'h': [256]*4, 'f': [[1024]]*4}\n",
    "model_graph_new = graph_util.model_dict_to_graph(model_dict_new)\n",
    "model_hash_new = graph_util.hash_graph(*model_graph_new)\n",
    "\n",
    "print(f'New model\\'s hash: {model_hash_new}')\n",
    "\n",
    "output_dir_new = f'./models/{model_hash_new}'\n",
    "\n",
    "chosen_neighbor_model = BertModelModular.from_pretrained(output_dir_bert_mini)\n",
    "\n",
    "# Finding the latest checkpoint for BERT-Mini\n",
    "PREFIX_CHECKPOINT_DIR = \"checkpoint\"\n",
    "_re_checkpoint = re.compile(r\"^\" + PREFIX_CHECKPOINT_DIR + r\"\\-(\\d+)$\")\n",
    "\n",
    "content = os.listdir(output_dir_bert_mini)\n",
    "checkpoints = [\n",
    "        path\n",
    "        for path in content\n",
    "        if _re_checkpoint.search(path) is not None and os.path.isdir(os.path.join(output_dir_bert_mini, path))\n",
    "    ]\n",
    "checkpoint_dir = max(checkpoints, key=lambda x: int(_re_checkpoint.search(x).groups()[0]))\n",
    "# print(checkpoint_dir)\n",
    "                \n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "config_new = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "config_new.from_model_dict_hetero(model_dict_new)\n",
    "    \n",
    "model_new = BertModelModular(config_new)\n",
    "model_new.load_model_from_source(chosen_neighbor_model)\n",
    "\n",
    "# Setting up checkpoint for new model\n",
    "if os.path.exists(output_dir_new):\n",
    "    shutil.rmtree(output_dir_new)\n",
    "shutil.copytree(os.path.join(output_dir_bert_mini, checkpoint_dir), os.path.join(output_dir_new, checkpoint_dir))\n",
    "os.remove(os.path.join(output_dir_new, checkpoint_dir, 'optimizer.pt'))\n",
    "# os.remove(os.path.join(output_dir_new, checkpoint_dir, 'scheduler.pt'))\n",
    "model_new.save_pretrained(os.path.join(output_dir_new, checkpoint_dir))\n",
    "\n",
    "args_train = get_training_args(0, 200, 32, output_dir_new, -1)\n",
    "\n",
    "metrics, log_history_model_new = pretrain(args_train, model_dict_bert_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss of grown model vs. original model with loss saturated\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot([state['step'] for state in log_history_bert_mini[:-1]], \n",
    "        [state['loss'] for state in log_history_bert_mini[:-1]], label='BERT-Mini')\n",
    "plt.plot([state['step'] for state in log_history_model_new[10:-1]], \n",
    "        [state['loss'] for state in log_history_model_new[10:-1]], label='Grown Model')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Training Steps')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'l_dft_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']], 'h': [256, 256, 256, 256], 'f': [[1024], [1024], [1024], [1024]]}\n",
      "{'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'c_13_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']], 'h': [256, 256, 256, 256], 'f': [[1024], [1024], [1024], [1024]]}\n",
      "{'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'c_13_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']], 'h': [256, 256, 256, 256], 'f': [[1024], [1024], [1024], [1024]]}\n",
      "{'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_wma_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']], 'h': [256, 256, 256, 256], 'f': [[1024], [1024], [1024], [1024]]}\n",
      "{'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']], 'h': [256, 256, 256, 256], 'f': [[1024], [1024], [1024], [1024]]}\n",
      "{'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'l_dct_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']], 'h': [256, 256, 256, 256], 'f': [[1024], [1024], [1024], [1024]]}\n",
      "{'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_wma_64']], 'h': [256, 256, 256, 256], 'f': [[1024], [1024], [1024], [1024]]}\n",
      "{'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_wma_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']], 'h': [256, 256, 256, 256], 'f': [[1024], [1024], [1024], [1024]]}\n",
      "{'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'c_5_64']], 'h': [256, 256, 256, 256], 'f': [[1024], [1024], [1024], [1024]]}\n",
      "{'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'c_13_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']], 'h': [256, 256, 256, 256], 'f': [[1024], [1024], [1024], [1024]]}\n"
     ]
    }
   ],
   "source": [
    "# Test automatic block-level growth\n",
    "model_dict = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']], \n",
    "                     'h': [256]*4, 'f': [[1024]]*4}\n",
    "\n",
    "ops = ['sa_sdp', 'sa_wma', 'l_dft', 'l_dct', 'c_5', 'c_9', 'c_13']\n",
    "\n",
    "num_samples = 10\n",
    "for sample_idx in range(num_samples):\n",
    "    layer = random.randint(0, model_dict['l']-1)\n",
    "    op = random.sample(ops, 1)[0]\n",
    "    \n",
    "    model_dict_new = deepcopy(model_dict)\n",
    "    \n",
    "    layer_hidden_dim = model_dict_new['o'][layer][0].split('_')[2]\n",
    "    model_dict_new['o'][layer].append(op + '_' +  layer_hidden_dim)\n",
    "    \n",
    "    print(model_dict_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1647] 2022-02-06 17:01:58,369 >> Didn't find file ../txf_design-space/roberta_tokenizer/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1647] 2022-02-06 17:01:58,369 >> Didn't find file ../txf_design-space/roberta_tokenizer/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-06 17:01:58,369 >> loading file ../txf_design-space/roberta_tokenizer/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-06 17:01:58,370 >> loading file ../txf_design-space/roberta_tokenizer/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-06 17:01:58,370 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-06 17:01:58,370 >> loading file ../txf_design-space/roberta_tokenizer/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-06 17:01:58,370 >> loading file ../txf_design-space/roberta_tokenizer/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-06 17:01:58,371 >> loading file None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0, Attention head 0, mean:  3.944e-04\n",
      "Layer 0, Attention head 1, mean:  3.976e-04\n",
      "Layer 0, Attention head 2, mean:  4.035e-04\n",
      "Layer 0, Attention head 3, mean:  1.147e-03\n",
      "Layer 1, Attention head 0, mean:  4.014e-04\n",
      "Layer 1, Attention head 1, mean:  3.998e-04\n",
      "Layer 1, Attention head 2, mean:  3.979e-04\n",
      "Layer 1, Attention head 3, mean:  3.986e-04\n",
      "Layer 2, Attention head 0, mean:  4.043e-04\n",
      "Layer 2, Attention head 1, mean:  4.009e-04\n",
      "Layer 2, Attention head 2, mean:  3.985e-04\n",
      "Layer 2, Attention head 3, mean:  3.991e-04\n",
      "Layer 3, Attention head 0, mean:  4.052e-04\n",
      "Layer 3, Attention head 1, mean:  3.987e-04\n",
      "Layer 3, Attention head 2, mean:  3.976e-04\n",
      "Layer 3, Attention head 3, mean:  3.990e-04\n"
     ]
    }
   ],
   "source": [
    "# Test block-level pruning\n",
    "model_dict = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_wma_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'c_5_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']], \n",
    "                     'h': [256]*4, 'f': [[1024, 1024, 1024]]*4}\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "\n",
    "config = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "config.from_model_dict_hetero(model_dict)\n",
    "\n",
    "model = BertForMaskedLMModular(config)\n",
    "\n",
    "for i in range(model_dict['l']):\n",
    "    attention_head_size = int(model_dict['o'][i][0].split('_')[2])\n",
    "    attention_layer = model.bert.encoder.layer[i].attention.self\n",
    "    wma_count, conv_count = 0, 0\n",
    "    for j in range(len(model_dict['o'][i])):\n",
    "        query_mean = torch.mean(torch.square(\n",
    "            attention_layer.query.weight[j*attention_head_size:(j+1)*attention_head_size])).item()\n",
    "        key_mean = torch.mean(torch.square(\n",
    "            attention_layer.key.weight[j*attention_head_size:(j+1)*attention_head_size])).item()\n",
    "        value_mean = torch.mean(torch.square(\n",
    "            attention_layer.value.weight[j*attention_head_size:(j+1)*attention_head_size])).item()\n",
    "        weights = [query_mean, key_mean, value_mean]\n",
    "        if model_dict['o'][i][j].split('_')[1] == 'wma':\n",
    "            wma_mean = torch.mean(torch.square(\n",
    "                getattr(attention_layer, f'W{wma_count}'))).item()\n",
    "            weights.append(wma_mean)\n",
    "            wma_count += 1\n",
    "        elif model_dict['o'][i][j].split('_')[1].isnumeric():\n",
    "            key_conv_attn_layer_mean = np.mean([torch.mean(torch.square(\n",
    "                getattr(attention_layer, f'key_conv_attn_layer{conv_count}').depthwise.weight)).item(),\n",
    "                                                torch.mean(torch.square(\n",
    "                getattr(attention_layer, f'key_conv_attn_layer{conv_count}').pointwise.weight)).item()])\n",
    "            conv_kernel_layer_mean = torch.mean(torch.square(\n",
    "                getattr(attention_layer, f'conv_kernel_layer{conv_count}').weight)).item()\n",
    "            conv_out_layer_mean = torch.mean(torch.square(\n",
    "                getattr(attention_layer, f'conv_out_layer{conv_count}').weight)).item()\n",
    "            conv_mean = np.mean([key_conv_attn_layer_mean, conv_kernel_layer_mean, conv_out_layer_mean])\n",
    "            weights.append(conv_mean)\n",
    "            conv_count += 1\n",
    "        print(f'Layer {i}, Attention head {j}, mean: {np.mean(weights): 0.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:658] 2022-02-06 17:03:49,645 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:581] 2022-02-06 17:03:49,646 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "[INFO|trainer.py:375] 2022-02-06 17:03:49,648 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:488] 2022-02-06 17:03:49,649 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLMModular.forward` and have been ignored: special_tokens_mask.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Trainer\n",
    "tokenized_datasets = load_from_disk(f'../../txf_design-space/tokenized_pretraining_dataset')\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "train_dataset = train_dataset.select(range(100))\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.1,\n",
    "    pad_to_multiple_of=None,\n",
    ")\n",
    "\n",
    "training_args = get_training_args(0, 1, 1, './models/test_model/', -1)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The model checkpoint for weights initialization.\"\n",
    "            \"Don't set if you want to train a model from scratch.\"\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    validation_split_percentage: Optional[int] = field(\n",
    "        default=5,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated.\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    mlm_probability: float = field(\n",
    "        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n",
    "    )\n",
    "    line_by_line: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_val_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of validation examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    '''\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, a json or a txt file.\"\n",
    "    '''\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "_, _, training_args = parser.parse_args_into_dataclasses(args_train)\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:658] 2022-02-06 17:17:12,223 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:581] 2022-02-06 17:17:12,223 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "[INFO|trainer.py:375] 2022-02-06 17:17:12,227 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:488] 2022-02-06 17:17:12,227 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLMModular.forward` and have been ignored: special_tokens_mask.\n",
      "[WARNING|training_args.py:633] 2022-02-06 17:17:12,228 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_grad.shape: (1024, 256)\n",
      "argmax(weight_grad): (16, 81)\n"
     ]
    }
   ],
   "source": [
    "# Test FFNN gradients\n",
    "if os.path.exists('./models/test_model/'):\n",
    "    shutil.rmtree('./models/test_model/')\n",
    "    \n",
    "args_train = get_training_args(0, 1, 1, './models/test_model/', -1)\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "_, _, args_train = parser.parse_args_into_dataclasses(args_train)\n",
    "set_seed(args_train.seed)\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args_train,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "for step, inputs in enumerate(train_dataloader):\n",
    "    model.train()\n",
    "    inputs = trainer._prepare_inputs(inputs)\n",
    "    loss = trainer.compute_loss(model, inputs)\n",
    "    loss.backward()\n",
    "    break\n",
    "    \n",
    "weight_grad = model.bert.encoder.layer[0].intermediate.sequential[0].weight.grad.cpu().numpy()\n",
    "print(f'weight_grad.shape: {weight_grad.shape}')\n",
    "weight_grad = np.abs(weight_grad)\n",
    "\n",
    "print(f'argmax(weight_grad): {np.unravel_index(np.argmax(weight_grad), weight_grad.shape)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txf_design-space [~/.conda/envs/txf_design-space/]",
   "language": "python",
   "name": "conda_txf_design-space"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
