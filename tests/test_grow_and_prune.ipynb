{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('../../txf_design-space/embeddings/utils')\n",
    "sys.path.append('../../txf_design-space/flexibert')\n",
    "sys.path.append('../grow_and_prune/')\n",
    "import graph_util\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import shlex\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from roberta_pretraining import pretrain\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset, interleave_datasets, load_from_disk\n",
    "from transformers.models.bert.modeling_modular_bert import BertModelModular, BertForMaskedLMModular\n",
    "from transformers import BertModel\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "\n",
    "import argparse\n",
    "import pretrain_model\n",
    "\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_MASKED_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.0.attention.self.distance_embedding.weight', 'roberta.encoder.layer.1.attention.self.distance_embedding.weight', 'roberta.encoder.layer.2.attention.self.distance_embedding.weight', 'roberta.encoder.layer.3.attention.self.distance_embedding.weight', 'roberta.encoder.layer.4.attention.self.distance_embedding.weight', 'roberta.encoder.layer.5.attention.self.distance_embedding.weight', 'roberta.encoder.layer.6.attention.self.distance_embedding.weight', 'roberta.encoder.layer.7.attention.self.distance_embedding.weight', 'roberta.encoder.layer.8.attention.self.distance_embedding.weight', 'roberta.encoder.layer.9.attention.self.distance_embedding.weight', 'roberta.encoder.layer.10.attention.self.distance_embedding.weight', 'roberta.encoder.layer.11.attention.self.distance_embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash of BERT-Base: 07aaba14d29455a984e2aef6312a8870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('../models/07aaba14d29455a984e2aef6312a8870/tokenizer_config.json',\n",
       " '../models/07aaba14d29455a984e2aef6312a8870/special_tokens_map.json',\n",
       " '../models/07aaba14d29455a984e2aef6312a8870/vocab.json',\n",
       " '../models/07aaba14d29455a984e2aef6312a8870/merges.txt',\n",
       " '../models/07aaba14d29455a984e2aef6312a8870/added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transfer weights of BERT-Base to heterogeneous counterpart\n",
    "roberta_base = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "model_dict_hetero = {'l': 12, 'o': [['sa_sdp_64']*12]*12, 'h': [768]*12, 'f': [[3072]]*12}\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "config = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "\n",
    "config.from_model_dict_hetero(model_dict_hetero)\n",
    "\n",
    "model = BertModelModular(config)\n",
    "\n",
    "model_state_dict = model.state_dict()\n",
    "model_state_dict.update(roberta_base.state_dict())\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "model_graph = graph_util.model_dict_to_graph(model_dict_hetero)\n",
    "model_hash = graph_util.hash_graph(*model_graph)\n",
    "output_dir = '../models/'+model_hash+'/'\n",
    "\n",
    "print(f'Hash of BERT-Base: {model_hash}')\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Model dict for BERT-Mini like model\n",
    "model_dict = {'l': 4, 'o': ['sa']*4, 'h': [256]*4, 'n': [4]*4, 'f': [[1024]]*4, 'p': ['sdp']*4}\n",
    "model_dict_hetero = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'c_5_64', 'sa_sdp_64']]*4, \n",
    "                     'h': [256]*4, 'f': [[1024]]*4}\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "\n",
    "config = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "# config.from_model_dict(model_dict)\n",
    "\n",
    "config.from_model_dict_hetero(model_dict_hetero)\n",
    "\n",
    "model = BertForMaskedLMModular(config)\n",
    "\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfering embeddings using mode: OD\n",
      "Checking layer 0...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: c_13_64\n",
      "\tTransfering attention head 3: sa_wma_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 1...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: c_13_64\n",
      "\tTransfering attention head 3: sa_wma_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 2...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: c_13_64\n",
      "\tTransfering attention head 3: sa_wma_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 3...\n",
      "\tTransfering distance embeddings using mode: OD\n",
      "\tTransfering attention head 0: sa_sdp_32\n",
      "\tTransfering attention head 1: sa_sdp_32\n",
      "\tTransfering attention head 2: c_5_32\n",
      "\tTransfering attention head 3: sa_wma_32\n",
      "\tTransfering feed-forward layer 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test load_from_source()\n",
    "# model_dict_hetero2 = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']]*3 + \\\n",
    "#                                    [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_wma_64']], \n",
    "#                       'h': [256]*4, 'f': [[1024, 1024]]*4}\n",
    "model_dict_hetero2 = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'c_13_64', 'sa_wma_64']]*3 + \\\n",
    "                                    [['sa_sdp_32', 'sa_sdp_32', 'c_5_32', 'sa_wma_32']], \n",
    "                     'h': [128, 256, 256, 512], 'f': [[2048, 256]]*4}\n",
    "# model_dict_hetero2 = {'l': 4, 'o': [['sa_sdp_32', 'sa_sdp_32', 'c_13_32', 'sa_sdp_32']]*4, \n",
    "#                      'h': [256]*4, 'f': [[1024]]*4}\n",
    "\n",
    "config2 = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "config2.from_model_dict_hetero(model_dict_hetero2)\n",
    "\n",
    "model2 = BertForMaskedLMModular(config2, transfer_mode='OD')\n",
    "\n",
    "# print(model2)\n",
    "\n",
    "model2.bert.load_model_from_source(model.bert, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.encoder.layer[0].attention.self.conv_kernel_layer0.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 128)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test RP\n",
    "from sklearn import random_projection\n",
    "\n",
    "matrix = torch.rand(1000, 1000)\n",
    "matrix_numpy = matrix.cpu().numpy()\n",
    "\n",
    "rp = random_projection.GaussianRandomProjection(128)\n",
    "matrix_numpy_new = rp.fit_transform(matrix_numpy)\n",
    "\n",
    "matrix_numpy_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RP with torch\n",
    "import torch.nn as nn\n",
    "from torch.tensor import Tensor\n",
    "\n",
    "model.bert.encoder.layer[0].attention.self.query.weight = \\\n",
    "    nn.parameter.Parameter(\n",
    "        torch.from_numpy(\n",
    "            rp.fit_transform(model.bert.encoder.layer[0].attention.self.query.weight.detach().numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), ['input', 'sa_h128_p-sdp', 'sa_h128_p-sdp', 'add_norm', 'f512', 'f512', 'add_norm', 'sa_h128_p-sdp', 'sa_h128_p-sdp', 'add_norm', 'f512', 'f512', 'add_norm', 'output'])\n"
     ]
    }
   ],
   "source": [
    "# Test graph generation from model dict\n",
    "model_dict = {'l': 2, 'o': ['sa']*2, 'h': [128]*2, 'n': [2]*2, 'f': [[512, 512]]*2, 'p': ['sdp']*2}\n",
    "\n",
    "model_graph = graph_util.model_dict_to_graph(model_dict)\n",
    "\n",
    "print(model_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50ef6167847c070825f29805f4e9cd35'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test hashing for model_dict_hetero\n",
    "model_dict_hetero = {'l': 4, 'o': [['l_dft_64', 'sa_wma_64', 'sa_wma_64', 'c_9_64']]*4, \n",
    "                     'h': [256]*4, 'n': [4]*4, 'f': [[1024]]*4}\n",
    "\n",
    "model_graph = graph_util.model_dict_to_graph(model_dict_hetero)\n",
    "\n",
    "graph_util.hash_graph(*model_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_args(seed, max_steps, per_gpu_batch_size, output_dir, local_rank):\n",
    "    a = \"--seed {} \\\n",
    "    --do_train \\\n",
    "    --max_seq_length 512 \\\n",
    "    --per_gpu_train_batch_size {} \\\n",
    "    --max_steps {} \\\n",
    "    --adam_epsilon 1e-6 \\\n",
    "    --adam_beta2 0.98 \\\n",
    "    --learning_rate 1000e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --warmup_steps 10000 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --output_dir {} \\\n",
    "    --local_rank {} \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 10 \\\n",
    "        \".format(seed, per_gpu_batch_size, max_steps, output_dir, local_rank)\n",
    "    return shlex.split(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT-Mini's model hash: 7f3d35e33a37f8d6a55d3d3ee339d7ba\n",
      "02/16/2022 17:18:54 - WARNING - roberta_pretraining -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "02/16/2022 17:18:54 - INFO - roberta_pretraining -   Training/evaluation parameters TrainingArguments(output_dir=./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/, overwrite_output_dir=False, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, eval_accumulation_steps=None, learning_rate=0.1, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.98, adam_epsilon=1e-06, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=100, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=10000, logging_dir=runs/Feb16_17-18-54_della-r2c3n1, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=10, save_strategy=IntervalStrategy.STEPS, save_steps=10, save_total_limit=2, no_cuda=False, seed=0, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name=./models/7f3d35e33a37f8d6a55d3d3ee339d7ba/, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=0, mp_parameters=)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1647] 2022-02-16 17:18:54,444 >> Didn't find file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1647] 2022-02-16 17:18:54,446 >> Didn't find file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-16 17:18:54,447 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-16 17:18:54,448 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-16 17:18:54,449 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-16 17:18:54,451 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-16 17:18:54,452 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-16 17:18:54,453 >> loading file None\n",
      "[INFO|trainer.py:375] 2022-02-16 17:19:00,998 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:488] 2022-02-16 17:19:00,999 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLMModular.forward` and have been ignored: special_tokens_mask.\n",
      "[WARNING|training_args.py:633] 2022-02-16 17:19:01,000 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:633] 2022-02-16 17:19:01,002 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:1011] 2022-02-16 17:19:01,002 >> ***** Running training *****\n",
      "[INFO|trainer.py:1012] 2022-02-16 17:19:01,002 >>   Num examples = 3149357\n",
      "[INFO|trainer.py:1013] 2022-02-16 17:19:01,003 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1014] 2022-02-16 17:19:01,003 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1015] 2022-02-16 17:19:01,003 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "[INFO|trainer.py:1016] 2022-02-16 17:19:01,003 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1017] 2022-02-16 17:19:01,004 >>   Total optimization steps = 100\n"
     ]
    }
   ],
   "source": [
    "# Test loss from manual GP\n",
    "model_dict_bert_mini = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']]*4, \n",
    "                     'h': [256]*4, 'f': [[1024]]*4}\n",
    "model_graph_bert_mini = graph_util.model_dict_to_graph(model_dict_bert_mini)\n",
    "model_hash_bert_mini = graph_util.hash_graph(*model_graph_bert_mini)\n",
    "\n",
    "print(f'BERT-Mini\\'s model hash: {model_hash_bert_mini}')\n",
    "\n",
    "output_dir_bert_mini = f'./models/{model_hash_bert_mini}/'\n",
    "\n",
    "args_train = get_training_args(0, 100, 32, output_dir_bert_mini, -1)\n",
    "\n",
    "if os.path.exists(output_dir_bert_mini):\n",
    "    shutil.rmtree(output_dir_bert_mini)\n",
    "\n",
    "metrics, log_history_bert_mini = pretrain(args_train, model_dict_bert_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loss after load_from_source of the same model\n",
    "model_dict_new = model_dict_bert_mini\n",
    "model_graph_new = graph_util.model_dict_to_graph(model_dict_new)\n",
    "model_hash_new = graph_util.hash_graph(*model_graph_new) + '_new'\n",
    "\n",
    "print(f'New model\\'s hash: {model_hash_new}')\n",
    "\n",
    "output_dir_new = f'./models/{model_hash_new}'\n",
    "\n",
    "chosen_neighbor_model = BertModelModular.from_pretrained(output_dir_bert_mini)\n",
    "\n",
    "# Finding the latest checkpoint for BERT-Mini\n",
    "PREFIX_CHECKPOINT_DIR = \"checkpoint\"\n",
    "_re_checkpoint = re.compile(r\"^\" + PREFIX_CHECKPOINT_DIR + r\"\\-(\\d+)$\")\n",
    "\n",
    "content = os.listdir(output_dir_bert_mini)\n",
    "checkpoints = [\n",
    "        path\n",
    "        for path in content\n",
    "        if _re_checkpoint.search(path) is not None and os.path.isdir(os.path.join(output_dir_bert_mini, path))\n",
    "    ]\n",
    "checkpoint_dir = max(checkpoints, key=lambda x: int(_re_checkpoint.search(x).groups()[0]))\n",
    "# print(checkpoint_dir)\n",
    "                \n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "config_new = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "config_new.from_model_dict_hetero(model_dict_new)\n",
    "    \n",
    "model_new = BertModelModular(config_new)\n",
    "model_new.load_model_from_source(chosen_neighbor_model)\n",
    "\n",
    "# Setting up checkpoint for new model\n",
    "if os.path.exists(output_dir_new):\n",
    "    shutil.rmtree(output_dir_new)\n",
    "shutil.copytree(os.path.join(output_dir_bert_mini, checkpoint_dir), os.path.join(output_dir_new, checkpoint_dir))\n",
    "os.remove(os.path.join(output_dir_new, checkpoint_dir, 'optimizer.pt'))\n",
    "# os.remove(os.path.join(output_dir_new, checkpoint_dir, 'scheduler.pt'))\n",
    "model_new.save_pretrained(os.path.join(output_dir_new, checkpoint_dir))\n",
    "\n",
    "args_train = get_training_args(0, 200, 32, output_dir_new, -1)\n",
    "\n",
    "metrics, log_history_bert_mini_new = pretrain(args_train, model_dict_bert_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loss from manual GP\n",
    "model_dict_new = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']]*3 + \\\n",
    "                  [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_wma_64']], \n",
    "        'h': [256]*4, 'f': [[1024]]*4}\n",
    "model_graph_new = graph_util.model_dict_to_graph(model_dict_new)\n",
    "model_hash_new = graph_util.hash_graph(*model_graph_new)\n",
    "\n",
    "print(f'New model\\'s hash: {model_hash_new}')\n",
    "\n",
    "output_dir_new = f'./models/{model_hash_new}'\n",
    "\n",
    "chosen_neighbor_model = BertModelModular.from_pretrained(output_dir_bert_mini)\n",
    "\n",
    "# Finding the latest checkpoint for BERT-Mini\n",
    "PREFIX_CHECKPOINT_DIR = \"checkpoint\"\n",
    "_re_checkpoint = re.compile(r\"^\" + PREFIX_CHECKPOINT_DIR + r\"\\-(\\d+)$\")\n",
    "\n",
    "content = os.listdir(output_dir_bert_mini)\n",
    "checkpoints = [\n",
    "        path\n",
    "        for path in content\n",
    "        if _re_checkpoint.search(path) is not None and os.path.isdir(os.path.join(output_dir_bert_mini, path))\n",
    "    ]\n",
    "checkpoint_dir = max(checkpoints, key=lambda x: int(_re_checkpoint.search(x).groups()[0]))\n",
    "# print(checkpoint_dir)\n",
    "                \n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "config_new = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "config_new.from_model_dict_hetero(model_dict_new)\n",
    "    \n",
    "model_new = BertModelModular(config_new)\n",
    "model_new.load_model_from_source(chosen_neighbor_model)\n",
    "\n",
    "# Setting up checkpoint for new model\n",
    "if os.path.exists(output_dir_new):\n",
    "    shutil.rmtree(output_dir_new)\n",
    "shutil.copytree(os.path.join(output_dir_bert_mini, checkpoint_dir), os.path.join(output_dir_new, checkpoint_dir))\n",
    "os.remove(os.path.join(output_dir_new, checkpoint_dir, 'optimizer.pt'))\n",
    "# os.remove(os.path.join(output_dir_new, checkpoint_dir, 'scheduler.pt'))\n",
    "model_new.save_pretrained(os.path.join(output_dir_new, checkpoint_dir))\n",
    "\n",
    "args_train = get_training_args(0, 200, 32, output_dir_new, -1)\n",
    "\n",
    "metrics, log_history_model_new = pretrain(args_train, model_dict_bert_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss of grown model vs. original model with loss saturated\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot([state['step'] for state in log_history_bert_mini[:-1]], \n",
    "        [state['loss'] for state in log_history_bert_mini[:-1]], label='BERT-Mini')\n",
    "plt.plot([state['step'] for state in log_history_model_new[10:-1]], \n",
    "        [state['loss'] for state in log_history_model_new[10:-1]], label='Grown Model')\n",
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Training Steps')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test automatic block-level growth\n",
    "model_dict = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']], \n",
    "                     'h': [256]*4, 'f': [[1024]]*4}\n",
    "\n",
    "ops = ['sa_sdp', 'sa_wma', 'l_dft', 'l_dct', 'c_5', 'c_9', 'c_13']\n",
    "\n",
    "num_samples = 10\n",
    "for sample_idx in range(num_samples):\n",
    "    layer = random.randint(0, model_dict['l']-1)\n",
    "    op = random.sample(ops, 1)[0]\n",
    "    \n",
    "    model_dict_new = deepcopy(model_dict)\n",
    "    \n",
    "    layer_hidden_dim = model_dict_new['o'][layer][0].split('_')[2]\n",
    "    model_dict_new['o'][layer].append(op + '_' +  layer_hidden_dim)\n",
    "    \n",
    "    print(model_dict_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test block-level pruning\n",
    "model_dict = {'l': 4, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_wma_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'c_5_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                           ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']], \n",
    "                     'h': [256]*4, 'f': [[1024, 1024, 1024]]*4}\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "\n",
    "config = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "config.from_model_dict_hetero(model_dict)\n",
    "\n",
    "model = BertForMaskedLMModular(config)\n",
    "\n",
    "for i in range(model_dict['l']):\n",
    "    attention_head_size = int(model_dict['o'][i][0].split('_')[2])\n",
    "    attention_layer = model.bert.encoder.layer[i].attention.self\n",
    "    wma_count, conv_count = 0, 0\n",
    "    for j in range(len(model_dict['o'][i])):\n",
    "        query_mean = torch.mean(torch.square(\n",
    "            attention_layer.query.weight[j*attention_head_size:(j+1)*attention_head_size])).item()\n",
    "        key_mean = torch.mean(torch.square(\n",
    "            attention_layer.key.weight[j*attention_head_size:(j+1)*attention_head_size])).item()\n",
    "        value_mean = torch.mean(torch.square(\n",
    "            attention_layer.value.weight[j*attention_head_size:(j+1)*attention_head_size])).item()\n",
    "        weights = [query_mean, key_mean, value_mean]\n",
    "        if model_dict['o'][i][j].split('_')[1] == 'wma':\n",
    "            wma_mean = torch.mean(torch.square(\n",
    "                getattr(attention_layer, f'W{wma_count}'))).item()\n",
    "            weights.append(wma_mean)\n",
    "            wma_count += 1\n",
    "        elif model_dict['o'][i][j].split('_')[1].isnumeric():\n",
    "            key_conv_attn_layer_mean = np.mean([torch.mean(torch.square(\n",
    "                getattr(attention_layer, f'key_conv_attn_layer{conv_count}').depthwise.weight)).item(),\n",
    "                                                torch.mean(torch.square(\n",
    "                getattr(attention_layer, f'key_conv_attn_layer{conv_count}').pointwise.weight)).item()])\n",
    "            conv_kernel_layer_mean = torch.mean(torch.square(\n",
    "                getattr(attention_layer, f'conv_kernel_layer{conv_count}').weight)).item()\n",
    "            conv_out_layer_mean = torch.mean(torch.square(\n",
    "                getattr(attention_layer, f'conv_out_layer{conv_count}').weight)).item()\n",
    "            conv_mean = np.mean([key_conv_attn_layer_mean, conv_kernel_layer_mean, conv_out_layer_mean])\n",
    "            weights.append(conv_mean)\n",
    "            conv_count += 1\n",
    "        print(f'Layer {i}, Attention head {j}, mean: {np.mean(weights): 0.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Trainer\n",
    "tokenized_datasets = load_from_disk(f'../../txf_design-space/tokenized_pretraining_dataset')\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "train_dataset = train_dataset.select(range(100))\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.1,\n",
    "    pad_to_multiple_of=None,\n",
    ")\n",
    "\n",
    "training_args = get_training_args(0, 1, 1, './models/test_model/', -1)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The model checkpoint for weights initialization.\"\n",
    "            \"Don't set if you want to train a model from scratch.\"\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    validation_split_percentage: Optional[int] = field(\n",
    "        default=5,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated.\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    mlm_probability: float = field(\n",
    "        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n",
    "    )\n",
    "    line_by_line: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_val_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of validation examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "    '''\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, a json or a txt file.\"\n",
    "    '''\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "_, _, training_args = parser.parse_args_into_dataclasses(args_train)\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test FFNN gradients\n",
    "if os.path.exists('./models/test_model/'):\n",
    "    shutil.rmtree('./models/test_model/')\n",
    "    \n",
    "args_train = get_training_args(0, 1, 1, './models/test_model/', -1)\n",
    "\n",
    "parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "_, _, args_train = parser.parse_args_into_dataclasses(args_train)\n",
    "set_seed(args_train.seed)\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args_train,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "for step, inputs in enumerate(train_dataloader):\n",
    "    model.train()\n",
    "    inputs = trainer._prepare_inputs(inputs)\n",
    "    loss = trainer.compute_loss(model, inputs)\n",
    "    loss.backward()\n",
    "    break\n",
    "    \n",
    "weight_grad = model.bert.encoder.layer[0].intermediate.sequential[0].weight.grad.cpu().numpy()\n",
    "print(f'weight_grad.shape: {weight_grad.shape}')\n",
    "weight_grad = np.abs(weight_grad)\n",
    "\n",
    "print(f'argmax(weight_grad): {np.unravel_index(np.argmax(weight_grad), weight_grad.shape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test weight gradients for neuron-level growth in FFNN\n",
    "\n",
    "models_dir = '../models'\n",
    "model_hash = '07aaba14d29455a984e2aef6312a8870'\n",
    "\n",
    "if os.path.exists(os.path.join(models_dir, 'test_models', model_hash)):\n",
    "    shutil.rmtree(os.path.join(models_dir, 'test_models', model_hash))\n",
    "shutil.copytree(os.path.join(models_dir, model_hash), os.path.join(models_dir, 'test_models', model_hash))\n",
    "\n",
    "# Get model dictionary\n",
    "model_dict = json.load(open(os.path.join(models_dir, 'test_models', model_hash, 'model_dict.json'), 'r'))\n",
    "\n",
    "# Get training arguments for pre-training\n",
    "def _get_training_args(seed, max_steps, per_gpu_batch_size, output_dir, local_rank):\n",
    "    a = \"--seed {} \\\n",
    "    --do_train \\\n",
    "    --max_seq_length 512 \\\n",
    "    --per_gpu_train_batch_size {} \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --max_steps {} \\\n",
    "    --adam_epsilon 1e-6 \\\n",
    "    --adam_beta2 0.98 \\\n",
    "    --learning_rate 1000e-4 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --warmup_steps 10000 \\\n",
    "    --lr_scheduler_type linear \\\n",
    "    --output_dir {} \\\n",
    "    --local_rank {} \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --logging_steps 10 \\\n",
    "    --save_steps 10 \\\n",
    "        \".format(seed, per_gpu_batch_size, max_steps, output_dir, local_rank)\n",
    "    return shlex.split(a)\n",
    "\n",
    "# Instantiate Trainer\n",
    "tokenized_datasets = load_from_disk(f'../../txf_design-space/tokenized_pretraining_dataset')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../../txf_design-space/roberta_tokenizer/')\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "train_dataset = train_dataset.select(range(100))\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm_probability=0.1,\n",
    "    pad_to_multiple_of=None,\n",
    ")\n",
    "\n",
    "# Get arguments for pre-training\n",
    "training_args = _get_training_args(0, 1, 16, os.path.join(models_dir, 'test_models', model_hash), -1)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(MODEL_FOR_MASKED_LM_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The model checkpoint for weights initialization.\"\n",
    "            \"Don't set if you want to train a model from scratch.\"\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    model_revision: str = field(\n",
    "        default=\"main\",\n",
    "        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "            \"with private models).\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    validation_split_percentage: Optional[int] = field(\n",
    "        default=5,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated.\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    mlm_probability: float = field(\n",
    "        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n",
    "    )\n",
    "    line_by_line: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_val_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of validation examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "local_parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n",
    "_, _, training_args = local_parser.parse_args_into_dataclasses(training_args)\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "# Instantiate model\n",
    "model = BertForMaskedLMModular.from_pretrained(os.path.join(models_dir, 'test_models', model_hash, 'checkpoint-72000'))\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "train_dataloader = trainer.get_train_dataloader()\n",
    "for step, inputs in enumerate(train_dataloader):\n",
    "    model.train()\n",
    "    inputs = trainer._prepare_inputs(inputs)\n",
    "    loss = trainer.compute_loss(model, inputs)\n",
    "    loss.backward()\n",
    "    break\n",
    "\n",
    "weight_grads = []\n",
    "\n",
    "for i in range(model_dict['l']):\n",
    "    for j in range(len(model_dict['f'][i])):\n",
    "        weight_grad = model.bert.encoder.layer[i].intermediate.sequential[j].weight.grad.cpu().numpy()\n",
    "        weight_grad = np.abs(weight_grad)\n",
    "\n",
    "        weight_grads.append({'layer': i, 'feed_forward_layer': j, 'weight_grad': np.mean(weight_grad)})\n",
    "  \n",
    "print(weight_grads)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test weight values for neuron-level pruning in FFNN\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    plt.plot(np.sort(np.mean(\n",
    "        np.abs(model.bert.encoder.layer[0].intermediate.sequential[0].weight.cpu().numpy()), axis=1)))\n",
    "plt.show()\n",
    "\n",
    "for i in range(model_dict['l']):\n",
    "    per_improvs = []\n",
    "    for j in range(len(model_dict['f'][i])):\n",
    "        with torch.no_grad():\n",
    "            weights_sorted = np.sort(np.mean(\n",
    "                np.abs(model.bert.encoder.layer[i].intermediate.sequential[0].weight.cpu().numpy()), axis=1))\n",
    "\n",
    "        weights_mean = np.mean(weights_sorted)\n",
    "        weights_pruned = np.mean(weights_sorted[16:])\n",
    "        \n",
    "        per_improv = (weights_pruned - weights_mean) / weights_mean\n",
    "        per_improvs.append(per_improv)\n",
    "\n",
    "    print(np.mean(per_improvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l': 12, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_wma_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64'], ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64']], 'h': [768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768, 768], 'f': [[3072], [3072], [3072], [3072], [3072], [3072], [3072], [3072], [3072], [3072], [3072], [3072]]}\n",
      "Loading embeddings directly\n",
      "Checking layer 0...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: sa_sdp_64\n",
      "\tTransfering attention head 3: sa_sdp_64\n",
      "\tTransfering attention head 4: sa_sdp_64\n",
      "\tTransfering attention head 5: sa_sdp_64\n",
      "\tTransfering attention head 6: sa_sdp_64\n",
      "\tTransfering attention head 7: sa_sdp_64\n",
      "\tTransfering attention head 8: sa_sdp_64\n",
      "\tTransfering attention head 9: sa_sdp_64\n",
      "\tTransfering attention head 10: sa_sdp_64\n",
      "\tTransfering attention head 11: sa_sdp_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 1...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: sa_sdp_64\n",
      "\tTransfering attention head 3: sa_sdp_64\n",
      "\tTransfering attention head 4: sa_sdp_64\n",
      "\tTransfering attention head 5: sa_sdp_64\n",
      "\tTransfering attention head 6: sa_sdp_64\n",
      "\tTransfering attention head 7: sa_sdp_64\n",
      "\tTransfering attention head 8: sa_sdp_64\n",
      "\tTransfering attention head 9: sa_sdp_64\n",
      "\tTransfering attention head 10: sa_sdp_64\n",
      "\tTransfering attention head 11: sa_sdp_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 2...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: sa_sdp_64\n",
      "\tTransfering attention head 3: sa_sdp_64\n",
      "\tTransfering attention head 4: sa_sdp_64\n",
      "\tTransfering attention head 5: sa_sdp_64\n",
      "\tTransfering attention head 6: sa_sdp_64\n",
      "\tTransfering attention head 7: sa_sdp_64\n",
      "\tTransfering attention head 8: sa_sdp_64\n",
      "\tTransfering attention head 9: sa_sdp_64\n",
      "\tTransfering attention head 10: sa_sdp_64\n",
      "\tTransfering attention head 11: sa_sdp_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 3...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: sa_sdp_64\n",
      "\tTransfering attention head 3: sa_sdp_64\n",
      "\tTransfering attention head 4: sa_sdp_64\n",
      "\tTransfering attention head 5: sa_sdp_64\n",
      "\tTransfering attention head 6: sa_sdp_64\n",
      "\tTransfering attention head 7: sa_sdp_64\n",
      "\tTransfering attention head 8: sa_sdp_64\n",
      "\tTransfering attention head 9: sa_sdp_64\n",
      "\tTransfering attention head 10: sa_sdp_64\n",
      "\tTransfering attention head 11: sa_sdp_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 4...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: sa_sdp_64\n",
      "\tTransfering attention head 3: sa_sdp_64\n",
      "\tTransfering attention head 4: sa_sdp_64\n",
      "\tTransfering attention head 5: sa_sdp_64\n",
      "\tTransfering attention head 6: sa_sdp_64\n",
      "\tTransfering attention head 7: sa_sdp_64\n",
      "\tTransfering attention head 8: sa_sdp_64\n",
      "\tTransfering attention head 9: sa_sdp_64\n",
      "\tTransfering attention head 10: sa_sdp_64\n",
      "\tTransfering attention head 11: sa_sdp_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 5...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: sa_sdp_64\n",
      "\tTransfering attention head 3: sa_sdp_64\n",
      "\tTransfering attention head 4: sa_sdp_64\n",
      "\tTransfering attention head 5: sa_sdp_64\n",
      "\tTransfering attention head 6: sa_sdp_64\n",
      "\tTransfering attention head 7: sa_sdp_64\n",
      "\tTransfering attention head 8: sa_sdp_64\n",
      "\tTransfering attention head 9: sa_sdp_64\n",
      "\tTransfering attention head 10: sa_sdp_64\n",
      "\tTransfering attention head 11: sa_sdp_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 6...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: sa_sdp_64\n",
      "\tTransfering attention head 3: sa_sdp_64\n",
      "\tTransfering attention head 4: sa_sdp_64\n",
      "\tTransfering attention head 5: sa_sdp_64\n",
      "\tTransfering attention head 6: sa_sdp_64\n",
      "\tTransfering attention head 7: sa_sdp_64\n",
      "\tTransfering attention head 8: sa_sdp_64\n",
      "\tTransfering attention head 9: sa_sdp_64\n",
      "\tTransfering attention head 10: sa_sdp_64\n",
      "\tTransfering attention head 11: sa_sdp_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 7...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: sa_sdp_64\n",
      "\tTransfering attention head 3: sa_sdp_64\n",
      "\tTransfering attention head 4: sa_sdp_64\n",
      "\tTransfering attention head 5: sa_sdp_64\n",
      "\tTransfering attention head 6: sa_sdp_64\n",
      "\tTransfering attention head 7: sa_sdp_64\n",
      "\tTransfering attention head 8: sa_sdp_64\n",
      "\tTransfering attention head 9: sa_sdp_64\n",
      "\tTransfering attention head 10: sa_sdp_64\n",
      "\tTransfering attention head 11: sa_sdp_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 8...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: sa_sdp_64\n",
      "\tTransfering attention head 3: sa_sdp_64\n",
      "\tTransfering attention head 4: sa_sdp_64\n",
      "\tTransfering attention head 5: sa_sdp_64\n",
      "\tTransfering attention head 6: sa_sdp_64\n",
      "\tTransfering attention head 7: sa_sdp_64\n",
      "\tTransfering attention head 8: sa_sdp_64\n",
      "\tTransfering attention head 9: sa_sdp_64\n",
      "\tTransfering attention head 10: sa_sdp_64\n",
      "\tTransfering attention head 11: sa_sdp_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 9...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: sa_sdp_64\n",
      "\tTransfering attention head 3: sa_sdp_64\n",
      "\tTransfering attention head 4: sa_sdp_64\n",
      "\tTransfering attention head 5: sa_sdp_64\n",
      "\tTransfering attention head 6: sa_sdp_64\n",
      "\tTransfering attention head 7: sa_sdp_64\n",
      "\tTransfering attention head 8: sa_sdp_64\n",
      "\tTransfering attention head 9: sa_sdp_64\n",
      "\tTransfering attention head 10: sa_sdp_64\n",
      "\tTransfering attention head 11: sa_sdp_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 10...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: sa_sdp_64\n",
      "\tTransfering attention head 3: sa_sdp_64\n",
      "\tTransfering attention head 4: sa_sdp_64\n",
      "\tTransfering attention head 5: sa_sdp_64\n",
      "\tTransfering attention head 6: sa_sdp_64\n",
      "\tTransfering attention head 7: sa_sdp_64\n",
      "\tTransfering attention head 8: sa_sdp_64\n",
      "\tTransfering attention head 9: sa_sdp_64\n",
      "\tTransfering attention head 10: sa_sdp_64\n",
      "\tTransfering attention head 11: sa_sdp_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Checking layer 11...\n",
      "\tLoading distace embeddings directly\n",
      "\tTransfering attention head 0: sa_sdp_64\n",
      "\tTransfering attention head 1: sa_sdp_64\n",
      "\tTransfering attention head 2: sa_sdp_64\n",
      "\tTransfering attention head 3: sa_sdp_64\n",
      "\tTransfering attention head 4: sa_sdp_64\n",
      "\tTransfering attention head 5: sa_sdp_64\n",
      "\tTransfering attention head 6: sa_sdp_64\n",
      "\tTransfering attention head 7: sa_sdp_64\n",
      "\tTransfering attention head 8: sa_sdp_64\n",
      "\tTransfering attention head 9: sa_sdp_64\n",
      "\tTransfering attention head 10: sa_sdp_64\n",
      "\tTransfering attention head 11: sa_sdp_64\n",
      "\tTransfering feed-forward layer 0\n",
      "Transfering MLM head\n",
      "\n",
      "Model key: bert.embeddings.position_ids is not transferred (or has 514 same weights)\n",
      "Model key: bert.encoder.layer.0.attention.self.W0 is not transferred (or has 4096 same weights)\n",
      "Model key: bert.encoder.layer.0.attention.self.query.weight is not transferred (or has 49152 same weights)\n",
      "Model key: bert.encoder.layer.0.attention.self.query.bias is not transferred (or has 64 same weights)\n",
      "Model key: bert.encoder.layer.0.attention.self.key.weight is not transferred (or has 49152 same weights)\n",
      "Model key: bert.encoder.layer.0.attention.self.key.bias is not transferred (or has 64 same weights)\n",
      "Model key: bert.encoder.layer.0.attention.self.value.weight is not transferred (or has 49152 same weights)\n",
      "Model key: bert.encoder.layer.0.attention.self.value.bias is not transferred (or has 64 same weights)\n",
      "Model key: bert.encoder.layer.0.attention.output.dense.weight is not transferred (or has 49152 same weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight transfer ratio: 0.9989547729492188\n",
      "Namespace(local_rank=-1, output_dir='../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7', steps=500)\n",
      "02/19/2022 16:33:54 - WARNING - roberta_pretraining -   Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "02/19/2022 16:33:54 - INFO - roberta_pretraining -   Training/evaluation parameters TrainingArguments(output_dir=../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7, overwrite_output_dir=False, do_train=True, do_eval=None, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=4, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.98, adam_epsilon=1e-06, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=100500, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=10000, logging_dir=runs/Feb19_16-33-54_della-i14g3, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=10, save_strategy=IntervalStrategy.STEPS, save_steps=10, save_total_limit=2, no_cuda=False, seed=0, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=10, dataloader_num_workers=0, past_index=-1, run_name=../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=[], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, _n_gpu=1, mp_parameters=)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1647] 2022-02-19 16:33:54,622 >> Didn't find file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1647] 2022-02-19 16:33:54,622 >> Didn't find file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-19 16:33:54,623 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/vocab.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-19 16:33:54,623 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/merges.txt\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-19 16:33:54,623 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-19 16:33:54,624 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-19 16:33:54,624 >> loading file /scratch/gpfs/stuli/txf_design-space/roberta_tokenizer/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1711] 2022-02-19 16:33:54,624 >> loading file None\n",
      "[INFO|trainer.py:375] 2022-02-19 16:34:01,359 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "[INFO|trainer.py:488] 2022-02-19 16:34:01,360 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLMModular.forward` and have been ignored: special_tokens_mask.\n",
      "[INFO|trainer.py:919] 2022-02-19 16:34:01,360 >> Loading model from ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100000).\n",
      "[INFO|configuration_utils.py:488] 2022-02-19 16:34:01,362 >> loading configuration file ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100000/config.json\n",
      "[INFO|configuration_utils.py:526] 2022-02-19 16:34:01,363 >> Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLMModular\"\n",
      "  ],\n",
      "  \"attention_heads_list\": [\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_wma_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ],\n",
      "    [\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\",\n",
      "      \"sa_sdp_64\"\n",
      "    ]\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"conv_kernel_size\": 9,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"ff_dim_list\": [\n",
      "    [\n",
      "      3072\n",
      "    ],\n",
      "    [\n",
      "      3072\n",
      "    ],\n",
      "    [\n",
      "      3072\n",
      "    ],\n",
      "    [\n",
      "      3072\n",
      "    ],\n",
      "    [\n",
      "      3072\n",
      "    ],\n",
      "    [\n",
      "      3072\n",
      "    ],\n",
      "    [\n",
      "      3072\n",
      "    ],\n",
      "    [\n",
      "      3072\n",
      "    ],\n",
      "    [\n",
      "      3072\n",
      "    ],\n",
      "    [\n",
      "      3072\n",
      "    ],\n",
      "    [\n",
      "      3072\n",
      "    ],\n",
      "    [\n",
      "      3072\n",
      "    ]\n",
      "  ],\n",
      "  \"from_model_dict_hetero\": true,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"head_ratio\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dim_list\": [\n",
      "    768,\n",
      "    768,\n",
      "    768,\n",
      "    768,\n",
      "    768,\n",
      "    768,\n",
      "    768,\n",
      "    768,\n",
      "    768,\n",
      "    768,\n",
      "    768,\n",
      "    768\n",
      "  ],\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_groups\": 1,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"relative_key\",\n",
      "  \"transformers_version\": \"4.6.0.dev0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1067] 2022-02-19 16:34:01,364 >> loading weights file ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100000/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1198] 2022-02-19 16:34:05,022 >> All model checkpoint weights were used when initializing BertForMaskedLMModular.\n",
      "\n",
      "[INFO|modeling_utils.py:1206] 2022-02-19 16:34:05,023 >> All the weights of BertForMaskedLMModular were initialized from the model checkpoint at ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLMModular for predictions without further training.\n",
      "[WARNING|training_args.py:633] 2022-02-19 16:34:05,091 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[WARNING|training_args.py:633] 2022-02-19 16:34:05,095 >> Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "[INFO|trainer.py:1011] 2022-02-19 16:34:05,095 >> ***** Running training *****\n",
      "[INFO|trainer.py:1012] 2022-02-19 16:34:05,095 >>   Num examples = 3149357\n",
      "[INFO|trainer.py:1013] 2022-02-19 16:34:05,096 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1014] 2022-02-19 16:34:05,096 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1015] 2022-02-19 16:34:05,096 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "[INFO|trainer.py:1016] 2022-02-19 16:34:05,096 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:1017] 2022-02-19 16:34:05,096 >>   Total optimization steps = 100500\n",
      "[INFO|trainer.py:1036] 2022-02-19 16:34:05,098 >>   Continuing training from checkpoint, will skip to saved global_step\n",
      "[INFO|trainer.py:1037] 2022-02-19 16:34:05,098 >>   Continuing training from epoch 2\n",
      "[INFO|trainer.py:1038] 2022-02-19 16:34:05,099 >>   Continuing training from global step 100000\n",
      "[INFO|trainer.py:1040] 2022-02-19 16:34:05,099 >>   Will skip the first 2 epochs then the first 6336 batches in the first epoch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100073' max='100500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100073/100500 03:18 < 19:51, 0.36 it/s, Epoch 2.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100010</td>\n",
       "      <td>1.347700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100020</td>\n",
       "      <td>1.318000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100030</td>\n",
       "      <td>1.347900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100040</td>\n",
       "      <td>1.341100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100050</td>\n",
       "      <td>1.329800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100060</td>\n",
       "      <td>1.313100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100070</td>\n",
       "      <td>1.322100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1667] 2022-02-19 16:36:47,666 >> Saving model checkpoint to ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100010\n",
      "[INFO|configuration_utils.py:329] 2022-02-19 16:36:47,670 >> Configuration saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100010/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-19 16:36:48,234 >> Model weights saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100010/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-19 16:36:48,235 >> tokenizer config file saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100010/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-19 16:36:48,235 >> Special tokens file saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100010/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-19 16:36:49,549 >> Deleting older checkpoint [../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-99500] due to args.save_total_limit\n",
      "[INFO|trainer.py:1667] 2022-02-19 16:37:15,587 >> Saving model checkpoint to ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100020\n",
      "[INFO|configuration_utils.py:329] 2022-02-19 16:37:15,588 >> Configuration saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100020/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-19 16:37:16,151 >> Model weights saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100020/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-19 16:37:16,152 >> tokenizer config file saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100020/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-19 16:37:16,153 >> Special tokens file saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100020/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-19 16:37:17,425 >> Deleting older checkpoint [../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100000] due to args.save_total_limit\n",
      "[INFO|trainer.py:1667] 2022-02-19 16:37:43,510 >> Saving model checkpoint to ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100030\n",
      "[INFO|configuration_utils.py:329] 2022-02-19 16:37:43,513 >> Configuration saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100030/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-19 16:37:44,079 >> Model weights saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100030/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-19 16:37:44,080 >> tokenizer config file saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100030/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-19 16:37:44,081 >> Special tokens file saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100030/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-19 16:37:45,381 >> Deleting older checkpoint [../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100010] due to args.save_total_limit\n",
      "[INFO|trainer.py:1667] 2022-02-19 16:38:11,507 >> Saving model checkpoint to ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100040\n",
      "[INFO|configuration_utils.py:329] 2022-02-19 16:38:11,508 >> Configuration saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100040/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-19 16:38:12,074 >> Model weights saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100040/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-19 16:38:12,076 >> tokenizer config file saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100040/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-19 16:38:12,076 >> Special tokens file saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100040/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-19 16:38:13,297 >> Deleting older checkpoint [../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100020] due to args.save_total_limit\n",
      "[INFO|trainer.py:1667] 2022-02-19 16:38:39,370 >> Saving model checkpoint to ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100050\n",
      "[INFO|configuration_utils.py:329] 2022-02-19 16:38:39,372 >> Configuration saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100050/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-19 16:38:39,954 >> Model weights saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100050/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-19 16:38:39,957 >> tokenizer config file saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100050/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-19 16:38:39,957 >> Special tokens file saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100050/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-19 16:38:41,289 >> Deleting older checkpoint [../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100030] due to args.save_total_limit\n",
      "[INFO|trainer.py:1667] 2022-02-19 16:39:07,426 >> Saving model checkpoint to ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100060\n",
      "[INFO|configuration_utils.py:329] 2022-02-19 16:39:07,428 >> Configuration saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100060/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-19 16:39:07,997 >> Model weights saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100060/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-19 16:39:07,998 >> tokenizer config file saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100060/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-19 16:39:07,998 >> Special tokens file saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100060/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-19 16:39:09,203 >> Deleting older checkpoint [../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100040] due to args.save_total_limit\n",
      "[INFO|trainer.py:1667] 2022-02-19 16:39:35,408 >> Saving model checkpoint to ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100070\n",
      "[INFO|configuration_utils.py:329] 2022-02-19 16:39:35,410 >> Configuration saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100070/config.json\n",
      "[INFO|modeling_utils.py:848] 2022-02-19 16:39:35,962 >> Model weights saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100070/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:1907] 2022-02-19 16:39:35,964 >> tokenizer config file saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100070/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:1913] 2022-02-19 16:39:35,965 >> Special tokens file saved in ../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100070/special_tokens_map.json\n",
      "[INFO|trainer.py:1734] 2022-02-19 16:39:37,212 >> Deleting older checkpoint [../models/8ee1fe926fcd3cb9d1775ce7c4f5f8d7/checkpoint-100050] due to args.save_total_limit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0087ace9773e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0mpretrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/gpfs/stuli/edge_txf/tests/../grow_and_prune/pretrain_model.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Pre-train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m# Save log history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gpfs/stuli/edge_txf/tests/../../txf_design-space/flexibert/roberta_pretraining.py\u001b[0m in \u001b[0;36mpretrain\u001b[0;34m(args, model_dict)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Saves the tokenizer too for easy upload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gpfs/stuli/txf_design-space/transformers/src/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gpfs/stuli/txf_design-space/transformers/src/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1516\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gpfs/stuli/txf_design-space/transformers/src/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1547\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1548\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1549\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gpfs/stuli/txf_design-space/transformers/src/transformers/models/bert/modeling_modular_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   2340\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2342\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   2343\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2344\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gpfs/stuli/txf_design-space/transformers/src/transformers/models/bert/modeling_modular_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1976\u001b[0m         )\n\u001b[1;32m   1977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1978\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1979\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1980\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gpfs/stuli/txf_design-space/transformers/src/transformers/models/bert/modeling_modular_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1355\u001b[0m                 )\n\u001b[1;32m   1356\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1358\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gpfs/stuli/txf_design-space/transformers/src/transformers/models/bert/modeling_modular_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    878\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gpfs/stuli/txf_design-space/transformers/src/transformers/models/bert/modeling_modular_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     ):\n\u001b[0;32m--> 777\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/txf_design-space/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/gpfs/stuli/txf_design-space/transformers/src/transformers/models/bert/modeling_modular_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mattention_scores_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mattention_scores_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0mwma_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test pretrain_model.py\n",
    "PREFIX_CHECKPOINT_DIR = \"checkpoint\"\n",
    "bert_hash = '07aaba14d29455a984e2aef6312a8870'\n",
    "\n",
    "models_dir = '../models/'\n",
    "model_dict_bert_base = json.load(open(os.path.join(models_dir, bert_hash, 'model_dict.json'), 'r'))\n",
    "model_dict = deepcopy(model_dict_bert_base)\n",
    "model_dict['o'][0].append('sa_wma_64')\n",
    "print(model_dict)\n",
    "\n",
    "model_graph = graph_util.model_dict_to_graph(model_dict)\n",
    "model_hash = graph_util.hash_graph(*model_graph)\n",
    "\n",
    "chosen_neighbor_path = os.path.join(models_dir, bert_hash)\n",
    "model_path = os.path.join(models_dir, model_hash)\n",
    "\n",
    "chosen_neighbor_model = BertForMaskedLMModular.from_pretrained(chosen_neighbor_path)\n",
    "\n",
    "# Finding the latest checkpoint for chosen neighbor\n",
    "re_checkpoint = re.compile(r\"^\" + PREFIX_CHECKPOINT_DIR + r\"\\-(\\d+)$\")\n",
    "content = os.listdir(chosen_neighbor_path)\n",
    "checkpoints = [\n",
    "        path\n",
    "        for path in content\n",
    "        if re_checkpoint.search(path) is not None and os.path.isdir(os.path.join(chosen_neighbor_path, path))\n",
    "    ]\n",
    "checkpoint_dir = max(checkpoints, key=lambda x: int(re_checkpoint.search(x).groups()[0]))\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('../txf_design-space/roberta_tokenizer/')\n",
    "config_new = BertConfig(vocab_size = tokenizer.vocab_size)\n",
    "config_new.from_model_dict_hetero(model_dict)\n",
    "\n",
    "# Transfer weights from chosen neighbor to the current model\n",
    "model = BertForMaskedLMModular(config_new, transfer_mode='RP')\n",
    "wt_ratio = model.load_model_from_source(chosen_neighbor_model, debug=True)\n",
    "print(f'Weight transfer ratio: {wt_ratio}')\n",
    "\n",
    "# Setting up checkpoint for the current model\n",
    "if os.path.exists(model_path):\n",
    "    shutil.rmtree(model_path)\n",
    "\n",
    "shutil.copytree(os.path.join(chosen_neighbor_path), os.path.join(model_path))\n",
    "\n",
    "try:\n",
    "    os.remove(os.path.join(model_path, checkpoint_dir, 'optimizer.pt'))\n",
    "    # os.remove(os.path.join(output_dir_new, checkpoint_dir, 'scheduler.pt'))\n",
    "except:\n",
    "    pass\n",
    "\n",
    "model.save_pretrained(os.path.join(model_path, checkpoint_dir))\n",
    "\n",
    "# Save model dictionary\n",
    "json.dump(model_dict, open(os.path.join(models_dir, model_hash, 'model_dict.json'), 'w+'))\n",
    "\n",
    "parser = argparse.ArgumentParser(\n",
    "    description='Input parameters for pretraining',\n",
    "    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "parser.add_argument('--output_dir',\n",
    "    metavar='',\n",
    "    type=str,\n",
    "    default=model_path,\n",
    "    help='path to save the pretrained model')\n",
    "parser.add_argument('--steps',\n",
    "    metavar='',\n",
    "    type=int,\n",
    "    default=500,\n",
    "    help='number of steps to pre-train beyond latest checkpoint')\n",
    "parser.add_argument('--local_rank',\n",
    "    metavar='',\n",
    "    type=int,\n",
    "    help='rank of the process during distributed training',\n",
    "    default=-1)\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "pretrain_model.main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model dictionary for BERT-Base\n",
    "model_dict_bert_base = {'l': 12, 'o': [['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64'], \\\n",
    "                                      ['sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', 'sa_sdp_64', \\\n",
    "                                       'sa_sdp_64', 'sa_sdp_64']], 'h': [768, 768, 768, 768, 768, 768, \\\n",
    "                                                                        768, 768, 768, 768, 768, 768], \\\n",
    "                                                                    'f': [[3072], [3072], [3072], \\\n",
    "                                                                        [3072], [3072], [3072], [3072], [3072], \\\n",
    "                                                                        [3072], [3072], [3072], [3072]]}\n",
    "json.dump(model_dict_bert_base, open(os.path.join(models_dir, bert_hash, 'model_dict.json'), 'w+'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "txf_design-space [~/.conda/envs/txf_design-space/]",
   "language": "python",
   "name": "conda_txf_design-space"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
